{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/cactus/source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/favicon-192x192.png","path":"images/favicon-192x192.png","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":1},{"_id":"themes/cactus/source/css/rtl.styl","path":"css/rtl.styl","modified":0,"renderable":1},{"_id":"themes/cactus/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/cactus/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/clipboard/clipboard.min.js","path":"lib/clipboard/clipboard.min.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff","path":"lib/vazir-font/Vazir-Black.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff","path":"lib/vazir-font/Vazir-Bold.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff2","path":"lib/vazir-font/Vazir-Black.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff2","path":"lib/vazir-font/Vazir-Bold.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff","path":"lib/vazir-font/Vazir-Light.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff2","path":"lib/vazir-font/Vazir-Light.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff","path":"lib/vazir-font/Vazir-Medium.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff2","path":"lib/vazir-font/Vazir-Medium.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff","path":"lib/vazir-font/Vazir-Thin.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff2","path":"lib/vazir-font/Vazir-Thin.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/font-face.css","path":"lib/vazir-font/font-face.css","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.woff2","path":"lib/vazir-font/Vazir.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/logo.png","path":"images/logo.png","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.woff","path":"lib/vazir-font/Vazir.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/jquery/jquery.min.js","path":"lib/jquery/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.ttf","path":"lib/vazir-font/Vazir-Black.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.eot","path":"lib/vazir-font/Vazir-Black.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.ttf","path":"lib/vazir-font/Vazir-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.eot","path":"lib/vazir-font/Vazir-Bold.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.eot","path":"lib/vazir-font/Vazir-Light.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.ttf","path":"lib/vazir-font/Vazir-Light.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.eot","path":"lib/vazir-font/Vazir-Medium.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.ttf","path":"lib/vazir-font/Vazir-Medium.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.eot","path":"lib/vazir-font/Vazir-Thin.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.ttf","path":"lib/vazir-font/Vazir-Thin.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.ttf","path":"lib/vazir-font/Vazir.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.eot","path":"lib/vazir-font/Vazir.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.eot","path":"lib/font-awesome/webfonts/fa-regular-400.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.ttf","path":"lib/font-awesome/webfonts/fa-regular-400.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff","path":"lib/font-awesome/webfonts/fa-regular-400.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/justified-gallery/css/justifiedGallery.min.css","path":"lib/justified-gallery/css/justifiedGallery.min.css","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/justified-gallery/js/jquery.justifiedGallery.min.js","path":"lib/justified-gallery/js/jquery.justifiedGallery.min.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff","path":"lib/font-awesome/webfonts/fa-brands-400.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.ttf","path":"lib/font-awesome/webfonts/fa-brands-400.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff","path":"lib/font-awesome/webfonts/fa-solid-900.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.eot","path":"lib/font-awesome/webfonts/fa-brands-400.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.svg","path":"lib/font-awesome/webfonts/fa-regular-400.svg","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.eot","path":"lib/font-awesome/webfonts/fa-solid-900.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGL-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.ttf","path":"lib/font-awesome/webfonts/fa-solid-900.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Italic.ttf","path":"lib/meslo-LG/MesloLGL-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Italic.ttf","path":"lib/meslo-LG/MesloLGM-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Italic.ttf","path":"lib/meslo-LG/MesloLGS-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGM-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGS-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Bold.ttf","path":"lib/meslo-LG/MesloLGM-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Bold.ttf","path":"lib/meslo-LG/MesloLGS-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Regular.ttf","path":"lib/meslo-LG/MesloLGM-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Regular.ttf","path":"lib/meslo-LG/MesloLGS-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Regular.ttf","path":"lib/meslo-LG/MesloLGL-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Bold.ttf","path":"lib/meslo-LG/MesloLGL-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.svg","path":"lib/font-awesome/webfonts/fa-brands-400.svg","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.svg","path":"lib/font-awesome/webfonts/fa-solid-900.svg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/cactus/.gitignore","hash":"c5345a2c5fa6c136dbe2020a405e541b4755a259","modified":1591422193978},{"_id":"themes/cactus/.jshintrc","hash":"2548bd6ce44422edc7e6f9f68061ab47f26c4f57","modified":1591422193978},{"_id":"themes/cactus/.stylintrc","hash":"eb5f48e83657928cb0cbee031373b2cd36ca0083","modified":1591422193978},{"_id":"themes/cactus/LICENSE","hash":"346ece39a983b0e7858c11f785cd846cef9eb875","modified":1591422193978},{"_id":"themes/cactus/gulpfile.js","hash":"0e55606323a45873506c08be6528478c08373e1e","modified":1591422193992},{"_id":"themes/cactus/README.md","hash":"812091a58d1d20467eede2d8b3d7bcf0fc0e8a50","modified":1591422193979},{"_id":"themes/cactus/_config.yml","hash":"e3c54e4d3f7ceb5e5d8c4708866e82d889aee1bf","modified":1591478036344},{"_id":"themes/cactus/package.json","hash":"a6060fadd36114d8cb74e7ff4c7d073901b5edcd","modified":1591422194008},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1591421788224},{"_id":"source/about/index.md","hash":"3a0b6df6c7e4f7794cb4065870a097778dd60ebf","modified":1591478887999},{"_id":"themes/cactus/languages/de.yml","hash":"43b2f4e078b042aaae0377a4235216a51ed82e0d","modified":1591422194001},{"_id":"themes/cactus/languages/ca.yml","hash":"b79dd2c21dc6697c635e92db1f661a4b8d5d2305","modified":1591422194000},{"_id":"themes/cactus/languages/en.yml","hash":"703548ad90034d4e5207a27eb50f726dc27e4c0c","modified":1591422194002},{"_id":"themes/cactus/languages/default.yml","hash":"703548ad90034d4e5207a27eb50f726dc27e4c0c","modified":1591422194002},{"_id":"themes/cactus/languages/fr.yml","hash":"5c07406998f19d219a5a7b65c0d88b6b023f85b2","modified":1591422194002},{"_id":"themes/cactus/languages/it.yml","hash":"62800bcae1f2d2454f87f4bcf4d7593848424f61","modified":1591422194002},{"_id":"themes/cactus/languages/kr.yml","hash":"651fb83991c91b13b53ed55740e5402cf0f1c5e8","modified":1591422194003},{"_id":"themes/cactus/languages/nl.yml","hash":"ac0573352ad2c737a7686bcca498b985e7bd6447","modified":1591422194003},{"_id":"themes/cactus/languages/fa.yml","hash":"63f32e50953af1c4bd0308a4fca5862b5287c2cb","modified":1591422194002},{"_id":"themes/cactus/languages/pl.yml","hash":"8a2d6dc874d86c38d42c2c861c39590647b5d536","modified":1591422194003},{"_id":"themes/cactus/languages/es.yml","hash":"2b1fc8b0d636123e9ee39017fa20053bd1913a5a","modified":1591422194002},{"_id":"themes/cactus/languages/pt-br.yml","hash":"4859aba788a050c2d5d0b997693b0c8c24b349f7","modified":1591422194003},{"_id":"themes/cactus/languages/ru.yml","hash":"81b57fcd1977ef534f4bf303dbc1b4710cc7f057","modified":1591422194003},{"_id":"themes/cactus/languages/tr.yml","hash":"2702914007e6bade9d6861078c0e179ac05bf48c","modified":1591422194004},{"_id":"themes/cactus/languages/vi.yml","hash":"f84893c3ec3e45875c90069e14b17ed3016ed973","modified":1591422194004},{"_id":"themes/cactus/languages/zh-CN.yml","hash":"8f81faaad9a0615b09dbc23868484a55ec958f6f","modified":1591422194004},{"_id":"themes/cactus/languages/zh-TW.yml","hash":"2f4e050c9b35a67f4a7278cec3a949533c2ac16a","modified":1591422194004},{"_id":"themes/cactus/layout/archive.ejs","hash":"53de8817e37be01b3ba8fa5ca31b9cafa2f3c011","modified":1591422194007},{"_id":"themes/cactus/layout/layout.ejs","hash":"8504004f2ed78914f806c6699d9bd722318cbe56","modified":1591422194008},{"_id":"themes/cactus/layout/page.ejs","hash":"c5465d5315a7544aa466b01fd8cfb62917a8bb1d","modified":1591422194008},{"_id":"themes/cactus/layout/post.ejs","hash":"a7d164ce888a60cd3eddd9c04bc6762428fa66bb","modified":1591422194008},{"_id":"themes/cactus/layout/index.ejs","hash":"1fd8aad25b2893a26b4483b91a341907e55c16be","modified":1591477031877},{"_id":"themes/cactus/scripts/merge-configs.js","hash":"2048c3415d96b17b9d84aa44bc0c25f1210525f8","modified":1591422194018},{"_id":"themes/cactus/scripts/meta.js","hash":"fa6055a39851c9953d033e70c1614547b94dce60","modified":1591422194029},{"_id":"themes/cactus/scripts/page_title.js","hash":"fa662dbdb82779af1b95e35ed7ccdf4866a53dee","modified":1591422194034},{"_id":"themes/cactus/scripts/thumbnail.js","hash":"df8829fd8c3119650037eba5ec11bdce06acff9d","modified":1591422194036},{"_id":"themes/cactus/layout/_partial/comments.ejs","hash":"4cf8d0059e5f8bc8ae1dd1a426293583fd398052","modified":1591422194004},{"_id":"themes/cactus/layout/_partial/footer.ejs","hash":"116424c97ec87f0d8124095e73d458eb6f4f12a0","modified":1591422194005},{"_id":"themes/cactus/layout/_partial/head.ejs","hash":"b7db191b7ad066b1f3f9c34d8a4b77e1ee815215","modified":1591422194005},{"_id":"themes/cactus/layout/_partial/header.ejs","hash":"6b534801486f6baa989bd351915a9156b838b777","modified":1591422194005},{"_id":"themes/cactus/layout/_partial/pagination.ejs","hash":"23bf862b3b8a3cd831850504d9b5a24d21b005e7","modified":1591422194005},{"_id":"themes/cactus/layout/_partial/scripts.ejs","hash":"18b9f77b9429722b54162add341e4d0b3de6a62a","modified":1591422194007},{"_id":"themes/cactus/layout/_partial/search.ejs","hash":"8b4bf9cf5db0ce762a31fc3baae0f2fc004bece4","modified":1591422194007},{"_id":"themes/cactus/layout/_partial/styles.ejs","hash":"be1b54388eb02176dd4722285dda19e3dce2e62e","modified":1591422194007},{"_id":"themes/cactus/source/images/apple-touch-icon.png","hash":"57e2def34682655f41a0be2d083f16765ba7858b","modified":1591422194088},{"_id":"themes/cactus/source/images/favicon-192x192.png","hash":"96e6fcbbb13a5914a6131391e210eb7dfd13d692","modified":1591422194096},{"_id":"themes/cactus/source/images/favicon.ico","hash":"189f9842bcb79a6f8f9e8445bc8bbd773443826b","modified":1591422194105},{"_id":"themes/cactus/source/css/_extend.styl","hash":"b6a4e5905a7515dda66919167531a5ab2b3d1fe2","modified":1591422194038},{"_id":"themes/cactus/source/css/_fonts.styl","hash":"354809b5a64e8a47a66c66fd1a28ac597c1460a6","modified":1591422194038},{"_id":"themes/cactus/source/css/_mixins.styl","hash":"1a9e309523df9685e8d088dcff0a809c58e2c392","modified":1591422194083},{"_id":"themes/cactus/source/css/_util.styl","hash":"2bfeb2e2605dd5235693b00c71a212646d2e0410","modified":1591422194087},{"_id":"themes/cactus/source/css/_variables.styl","hash":"02079fb71b7d1c01d15fa512a1948ad4cbb416b5","modified":1591422194087},{"_id":"themes/cactus/source/css/rtl.styl","hash":"ff8700e1626feeb53d905a2df2777bda7d1eca50","modified":1591422194088},{"_id":"themes/cactus/source/css/style.styl","hash":"18b22cfdc7457d81db7694aef5850cc36ff87a77","modified":1591422194088},{"_id":"themes/cactus/source/js/main.js","hash":"584c5a69ac81a483a1c4377a2e2cf326c2795e7b","modified":1591422194129},{"_id":"themes/cactus/source/js/search.js","hash":"62df6eeb66e002a600317a288e3f8fcd0e9c5492","modified":1591422194130},{"_id":"themes/cactus/layout/_partial/post/actions_mobile.ejs","hash":"79b234ff3c264e66b2e71c819228e62bf92b48e4","modified":1591422194006},{"_id":"themes/cactus/layout/_partial/post/category.ejs","hash":"b5bfa049f17868fb09d9d2a7e1d5279fa0381d37","modified":1591422194006},{"_id":"themes/cactus/layout/_partial/post/date.ejs","hash":"6f2d1aa9562df343b797d25705f1945323c465fb","modified":1591422194006},{"_id":"themes/cactus/layout/_partial/post/gallery.ejs","hash":"9aecd8908e8a684f33dc20c02497c0f1774137c7","modified":1591422194006},{"_id":"themes/cactus/layout/_partial/post/share.ejs","hash":"1a294382bd14d979525b8ed934d807bc7d083e4d","modified":1591422194006},{"_id":"themes/cactus/layout/_partial/post/tag.ejs","hash":"e08fae30da060f49c087f6c121868b08eb55c795","modified":1591422194007},{"_id":"themes/cactus/layout/_partial/post/title.ejs","hash":"a060f1c6e3718494a6b1d0e1981ea0bf4e549828","modified":1591422194007},{"_id":"themes/cactus/source/css/_colors/dark.styl","hash":"9c9655b42b85f754b8a573a1d4634c23c680e1bf","modified":1591422194037},{"_id":"themes/cactus/source/css/_colors/white.styl","hash":"2b25ad24573bded8b42f9d80112eab9fadbed1a5","modified":1591422194038},{"_id":"themes/cactus/source/css/_colors/classic.styl","hash":"0f0ec41a4165814ce69688425d5ac4d701b7cc70","modified":1591422194037},{"_id":"themes/cactus/source/css/_colors/light.styl","hash":"d09f781cb02394850737b3a9efc6693307d5bf09","modified":1591422194038},{"_id":"themes/cactus/source/css/_highlight/agate.styl","hash":"53027913ed8d4f75ac3e49e76aad824f0df62da3","modified":1591422194038},{"_id":"themes/cactus/source/css/_highlight/androidstudio.styl","hash":"2af0861725f97f0ee2ded67c3d2d4548c62b2d16","modified":1591422194039},{"_id":"themes/cactus/source/css/_highlight/arduino-light.styl","hash":"15e8572585cd708221c513dea4bdd89d8fe56c10","modified":1591422194039},{"_id":"themes/cactus/source/css/_highlight/arta.styl","hash":"b3e81e3e694ceb8deed178adb8b91013c5120e30","modified":1591422194039},{"_id":"themes/cactus/source/css/_highlight/ascetic.styl","hash":"32cff3bef6fac3760fe78f203096477052a90552","modified":1591422194039},{"_id":"themes/cactus/source/css/_highlight/atelier-cave-dark.styl","hash":"ce63dd8548688d88254405eedfa75b1d7c82449e","modified":1591422194039},{"_id":"themes/cactus/source/css/_highlight/atelier-cave-light.styl","hash":"a5be0744a7ecf4a08f600ade4cfd555afc67bc15","modified":1591422194040},{"_id":"themes/cactus/source/css/_highlight/atelier-dune-dark.styl","hash":"c196ff0ee064af0e507823694ae39020addfc280","modified":1591422194040},{"_id":"themes/cactus/source/css/_highlight/atelier-dune-light.styl","hash":"931435fbc6f974e8ce9e32722680035d248a9dc1","modified":1591422194040},{"_id":"themes/cactus/source/css/_highlight/atelier-estuary-dark.styl","hash":"0bb16a4eff93688f40787abc2f9e56e7d5cc93e7","modified":1591422194040},{"_id":"themes/cactus/source/css/_highlight/atelier-estuary-light.styl","hash":"344276ca9b27e51d4c907f76afe5d13cf8e60bdf","modified":1591422194040},{"_id":"themes/cactus/source/css/_highlight/atelier-forest-dark.styl","hash":"effbc5d75fa87203c847039869c22031b40d5b7d","modified":1591422194040},{"_id":"themes/cactus/source/css/_highlight/atelier-forest-light.styl","hash":"95228d9f2102fad425536aac44b80b2cba1f5950","modified":1591422194041},{"_id":"themes/cactus/source/css/_highlight/atelier-heath-dark.styl","hash":"9a2e9a1d0a01bbdf158560c3ed1c134e098b2c68","modified":1591422194041},{"_id":"themes/cactus/source/css/_highlight/atelier-heath-light.styl","hash":"8c8c2e445abef85273be966d59770e9ced6aac21","modified":1591422194041},{"_id":"themes/cactus/source/css/_highlight/atelier-lakeside-dark.styl","hash":"10ee3882fca7b97a37bd309d2d35fce9868647bb","modified":1591422194041},{"_id":"themes/cactus/source/css/_highlight/atelier-lakeside-light.styl","hash":"2c54cb9bdb259ae3b5b29f63ac2469ed34b08578","modified":1591422194041},{"_id":"themes/cactus/source/css/_highlight/atelier-plateau-dark.styl","hash":"84c80e6f67f62fce958d25817c277d2360272617","modified":1591422194041},{"_id":"themes/cactus/source/css/_highlight/atelier-plateau-light.styl","hash":"d1a05fdd1ededc9063d181ab25bad55a164aeb4a","modified":1591422194042},{"_id":"themes/cactus/source/css/_highlight/atelier-savanna-dark.styl","hash":"e32c1c70def8060fce5e790979a126da650ac642","modified":1591422194042},{"_id":"themes/cactus/layout/_partial/post/actions_desktop.ejs","hash":"38aadd1ed890303dde582b722486138afee09b0a","modified":1591422194006},{"_id":"themes/cactus/source/css/_highlight/atelier-seaside-dark.styl","hash":"2edf385215bbe1985b1a10106525d362667d28c2","modified":1591422194042},{"_id":"themes/cactus/source/css/_highlight/atelier-savanna-light.styl","hash":"f8244c93711c7cb59dd79d2df966806b30d171ea","modified":1591422194042},{"_id":"themes/cactus/source/css/_highlight/atelier-sulphurpool-dark.styl","hash":"538a14321193cd8abf2ddc484306631e54149ffb","modified":1591422194043},{"_id":"themes/cactus/source/css/_highlight/atelier-seaside-light.styl","hash":"0597342da6e2d0c5bdcc7d42dabb07322b1a4177","modified":1591422194042},{"_id":"themes/cactus/source/css/_highlight/atelier-sulphurpool-light.styl","hash":"efa52713efc468abeeb2b9299704371583b857de","modified":1591422194043},{"_id":"themes/cactus/source/css/_highlight/brown-paper.styl","hash":"c2326ba20a5020a66ca7895258d18833327d4334","modified":1591422194043},{"_id":"themes/cactus/source/css/_highlight/codepen-embed.styl","hash":"8b7b34484f76a6c2c3b1a9e49abb9b382f439ae8","modified":1591422194051},{"_id":"themes/cactus/source/css/_highlight/color-brewer.styl","hash":"2a439d6214430e2f45dd4939b4dfe1fe1a20aa0f","modified":1591422194052},{"_id":"themes/cactus/source/css/_highlight/dark.styl","hash":"f5e6e75958de59e87fc6be3a1668e870e20bc836","modified":1591422194052},{"_id":"themes/cactus/source/css/_highlight/darkula.styl","hash":"9717efa9194837ba3fb4d762997d33075dcf8bfa","modified":1591422194052},{"_id":"themes/cactus/source/css/_highlight/docco.styl","hash":"b1c176378bb275f2e8caa759f36294e42d614bf1","modified":1591422194052},{"_id":"themes/cactus/source/css/_highlight/far.styl","hash":"aaac3028f5e33123cd123a583cddc9290c45ec8e","modified":1591422194052},{"_id":"themes/cactus/source/css/_highlight/foundation.styl","hash":"bf8ddc94b4ad995b8b8805b5a4cf95004553fdac","modified":1591422194052},{"_id":"themes/cactus/source/css/_highlight/github-gist.styl","hash":"48211a03d33e7f7ada0b261162bea06676155a71","modified":1591422194053},{"_id":"themes/cactus/source/css/_highlight/github.styl","hash":"3336aeba324c6d34a6fd41fef9b47bc598f7064c","modified":1591422194053},{"_id":"themes/cactus/source/css/_highlight/googlecode.styl","hash":"bda816beee7b439814b514e6869dc678822be1bc","modified":1591422194053},{"_id":"themes/cactus/source/css/_highlight/grayscale.styl","hash":"bf37d8b8d1e602126c51526f0cc28807440228ed","modified":1591422194053},{"_id":"themes/cactus/source/css/_highlight/gruvbox-dark.styl","hash":"76b744c14fd5600bea64731c05df97c2df75523f","modified":1591422194053},{"_id":"themes/cactus/source/css/_highlight/highlightjs.styl","hash":"0e198b7a59191c7a39b641a4ddd22c948edb9358","modified":1591422194054},{"_id":"themes/cactus/source/css/_highlight/hopscotch.styl","hash":"1378a6bc67a32c0cbff72ab771268b53f9aa586d","modified":1591422194054},{"_id":"themes/cactus/source/css/_highlight/hybrid.styl","hash":"b8eb5c69d12f2ee5ebc50265ae271699d7f1a8d3","modified":1591422194054},{"_id":"themes/cactus/source/css/_highlight/idea.styl","hash":"a02967cb51c16a34e0ee895d33ded2b823d35b21","modified":1591422194054},{"_id":"themes/cactus/source/css/_highlight/index.styl","hash":"002d5596f6379cc87dbd43d9145bc764aa666be1","modified":1591422194054},{"_id":"themes/cactus/source/css/_highlight/ir-black.styl","hash":"53e5d74326a4527b92272bbd6946d4fec92720e8","modified":1591422194054},{"_id":"themes/cactus/source/css/_highlight/kimbie.dark.styl","hash":"45dbb168f22d739d0109745d2decd66b5f94e786","modified":1591422194055},{"_id":"themes/cactus/source/css/_highlight/kimbie.styl","hash":"51b889ca7c6fe178cfbbe28d875a6ea427184441","modified":1591422194055},{"_id":"themes/cactus/source/css/_highlight/magula.styl","hash":"16d323f989b1420a0f72ef989242ece9bf17a456","modified":1591422194055},{"_id":"themes/cactus/source/css/_highlight/monokai-sublime.styl","hash":"c385b11345894be7e6ce3c5f08663e199933b8e4","modified":1591422194056},{"_id":"themes/cactus/source/css/_highlight/monokai.styl","hash":"f87be027848ea6bee623a08ad1e17b2f5b7937ee","modified":1591422194056},{"_id":"themes/cactus/source/css/_highlight/kimbie.light.styl","hash":"61f8baed25be05288c8604d5070afbcd9f183f49","modified":1591422194055},{"_id":"themes/cactus/source/css/_highlight/mono-blue.styl","hash":"4c89a6ae29de67c0700585af82a60607e85df928","modified":1591422194055},{"_id":"themes/cactus/source/css/_highlight/obsidian.styl","hash":"199e28326be8590883f0813ebbd54fcfaa4750fd","modified":1591422194056},{"_id":"themes/cactus/source/css/_highlight/paraiso-dark.styl","hash":"f1537bd868579fa018ecdbfd2eb922dcf3ba2cac","modified":1591422194056},{"_id":"themes/cactus/source/css/_highlight/paraiso-light.styl","hash":"d224d1df0eb3395d9eea1344cee945c228af2911","modified":1591422194056},{"_id":"themes/cactus/source/css/_highlight/paraiso.styl","hash":"75f181eece6b71d033ea0c8d6cf00ae7efb9e29b","modified":1591422194057},{"_id":"themes/cactus/source/css/_highlight/pojoaque.jpg","hash":"c5fe6533b88b21f8d90d3d03954c6b29baa67791","modified":1591422194057},{"_id":"themes/cactus/source/css/_highlight/pojoaque.styl","hash":"4e7b6b046b8575ac749f6aec4e953a62ada27a36","modified":1591422194070},{"_id":"themes/cactus/source/css/_highlight/railscasts.styl","hash":"b6674db9210e0c4444e4835fff2d1361f3ebd64c","modified":1591422194070},{"_id":"themes/cactus/source/css/_highlight/rainbow.styl","hash":"c0cf97aae3e10fdcd10414547a711c9effbc39b8","modified":1591422194071},{"_id":"themes/cactus/source/css/_highlight/school-book.styl","hash":"d43560fe519a931ce6da7d57416d7aa148441b83","modified":1591422194080},{"_id":"themes/cactus/source/css/_highlight/solarized-dark.styl","hash":"90c9da5aa594383697e5b18892a7f95beb053f55","modified":1591422194080},{"_id":"themes/cactus/source/css/_highlight/solarized-light.styl","hash":"aa0dd3fd25c464183b59c5575c9bee8756b397f2","modified":1591422194080},{"_id":"themes/cactus/source/css/_highlight/sunburst.styl","hash":"af3eec0fd56151e55bbd49c31b151f36717611d8","modified":1591422194081},{"_id":"themes/cactus/source/css/_highlight/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1591422194071},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-blue.styl","hash":"f24c17d0ab815dcfaab3438cb9fe2ab4839f5e0d","modified":1591422194081},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-bright.styl","hash":"7674fecb6d27350727dc0d2dc93bc018382ebbd0","modified":1591422194081},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-eighties.styl","hash":"28d751075ebabf7d0327a36f725076fe82fdf626","modified":1591422194081},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night.styl","hash":"16ba09b2db501e4e3e2e7d62595d9bf935bf27c4","modified":1591422194082},{"_id":"themes/cactus/source/css/_highlight/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1591422194043},{"_id":"themes/cactus/source/css/_highlight/vs.styl","hash":"959a746f4b37aacb5d1d6ff1d57e0c045289d75d","modified":1591422194082},{"_id":"themes/cactus/source/css/_highlight/tomorrow.styl","hash":"15779cf6846725c7c35fc56cac39047d7e0aec1c","modified":1591422194082},{"_id":"themes/cactus/source/css/_highlight/xcode.styl","hash":"5e8532ae8366dcf6a4ef5e4813dc3d42ab3d0a50","modified":1591422194082},{"_id":"themes/cactus/source/css/_partial/archive.styl","hash":"ef8fc52337c4c7b010cad7c427cb78009b30f9d8","modified":1591422194083},{"_id":"themes/cactus/source/css/_highlight/zenburn.styl","hash":"68ff9332ccc03f9389b15b713415cde016f8088f","modified":1591422194083},{"_id":"themes/cactus/source/css/_partial/article.styl","hash":"c6a3c395ceb4aacba8c995bcde7b58a7ca501919","modified":1591422194083},{"_id":"themes/cactus/source/css/_partial/categories.styl","hash":"a43f00e61b3507f130b8a3f8108a4eeca147c2a0","modified":1591422194084},{"_id":"themes/cactus/source/css/_partial/comments.styl","hash":"1e90f1fb9d4c155df518cacb5a537e9de9c042c1","modified":1591422194084},{"_id":"themes/cactus/source/css/_partial/footer.styl","hash":"14dda7f155bb21e6cd33ca3d8daa5b489b4707b3","modified":1591422194084},{"_id":"themes/cactus/source/css/_partial/index.styl","hash":"59c99f4ea3a73bf47ce030df166c5e33d5de31fb","modified":1591422194085},{"_id":"themes/cactus/source/css/_partial/pagination.styl","hash":"950bf517bbe7adb9a9aa4eb5ddec74ffc7598787","modified":1591422194085},{"_id":"themes/cactus/source/css/_partial/header.styl","hash":"519af79eb34ee922b48e6c19aa8f4856e3f76486","modified":1591422194085},{"_id":"themes/cactus/source/css/_partial/search.styl","hash":"159be002780c62a77f46947cf854a7342fba24f4","modified":1591422194086},{"_id":"themes/cactus/source/css/_partial/tags.styl","hash":"d571d5c7c960300d29c5f0ec3fe1140322ecd6b3","modified":1591422194086},{"_id":"themes/cactus/source/css/_partial/tooltip.styl","hash":"2daff581ec3efaec840cbfdee512195919c32629","modified":1591422194087},{"_id":"themes/cactus/source/lib/clipboard/clipboard.min.js","hash":"ee60ca5ba9401456105ef703a98092369b579c80","modified":1591422194132},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff","hash":"9e8d954c46eaad8b8234fa906e9a268ee354dced","modified":1591422194203},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff","hash":"9376516725e502f4375e06cc4fa7d940e2c93251","modified":1591422194205},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff2","hash":"f1c5d7523d21c2bf820d827c9d5df4184c3866dc","modified":1591422194203},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff2","hash":"8d4810991aa94f958aff20a9cd381caf338e3061","modified":1591422194206},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff","hash":"cba77d3d16f7565f9ea79bd7657f4e00c7fe851f","modified":1591422194208},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff2","hash":"e520d5f6bf7ea3c1e4f2aef2abbbc6a6f9b697cb","modified":1591422194208},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff","hash":"2e8e3c873e6d98acc3c10aa84997104b276e1e68","modified":1591422194210},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff2","hash":"976b7aa7c2c2c049c548a25b5914cfbda74b0453","modified":1591422194210},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff","hash":"f231111b3c778b7a5898ea88c0f150c0e72be468","modified":1591422194212},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff2","hash":"baa42f95b41411b9aeaa6c7594e5ccee10d42ac4","modified":1591422194212},{"_id":"themes/cactus/source/lib/vazir-font/font-face.css","hash":"2a95709b15ee45fc2328051038ceedddf83235bb","modified":1591422194214},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.woff2","hash":"dc3c0ed67c9abb062b562e8553776f614d2946c2","modified":1591422194214},{"_id":"themes/cactus/source/images/logo.png","hash":"0e3029251dfda26adee2761f71377297e8c26871","modified":1591422194107},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.woff","hash":"c798391d624b9bb44497a87ffc4f7eb52042dceb","modified":1591422194214},{"_id":"themes/cactus/source/lib/jquery/jquery.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1591422194156},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.ttf","hash":"23ee4a19421de9a0ca9dddc5435a8efe5bf28d87","modified":1591422194202},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.eot","hash":"603acd29416644e4b4fb8646abeada1865ba6869","modified":1591422194201},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.ttf","hash":"658c1da4f2a0124f6340058daa6873a86e6ba4fc","modified":1591422194205},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.eot","hash":"9ef82b07f3adad7d644c9c3a6d35a0c727bd64e8","modified":1591422194204},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.eot","hash":"32f51bf715663f5ca419e138617fc05f7055aef5","modified":1591422194207},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.ttf","hash":"b2372b67b9519fb4fa8e05de6c0ddae56845ff79","modified":1591422194207},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.eot","hash":"17be9f699c30f0384004b46e991db8ac38a9992e","modified":1591422194209},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.ttf","hash":"31cdbcc7215d01c9dd2beb8a28f8b7a7de75b9f4","modified":1591422194209},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.eot","hash":"2c0ebb336dd012da8d575cae0ee4d048b65fe6e1","modified":1591422194211},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.ttf","hash":"d53fcc2e2d6c9c77613afcd34058be1b75bb0fef","modified":1591422194211},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.ttf","hash":"c7a3f2f4d56c4c4ec69d395baf39b55198da0254","modified":1591422194213},{"_id":"themes/cactus/source/lib/vazir-font/Vazir.eot","hash":"bd3a7cb9eb70d36b4cfba8c5a05d234aefeefe3c","modified":1591422194213},{"_id":"themes/cactus/source/css/_partial/post/actions_mobile.styl","hash":"0d2966c1d870392476864af8ee3ba312ba30cb82","modified":1591422194086},{"_id":"themes/cactus/source/css/_partial/post/actions_desktop.styl","hash":"ae3d9090bf4d934d443c0a431cb09d009743155a","modified":1591422194085},{"_id":"themes/cactus/source/lib/font-awesome/css/all.min.css","hash":"703603273f5d5d52eb456d6385e1a68294fbd568","modified":1591422194134},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.eot","hash":"3ad44eb5c276d1435408f253ca78da729a1aca90","modified":1591422194142},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.ttf","hash":"114f35e6d9053caca2ef6d1e34fea3f87a59245b","modified":1591422194143},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"830f8653e5f4a5331ac0b47c5701f65fe9f1bb32","modified":1591422194144},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff","hash":"f3f0ea4847825806062a9b7a0f629671eb6b6408","modified":1591422194144},{"_id":"themes/cactus/source/lib/justified-gallery/css/justifiedGallery.min.css","hash":"92bb6e468a1db7fbd99ccb960e15e28572254263","modified":1591422194165},{"_id":"themes/cactus/source/lib/justified-gallery/js/jquery.justifiedGallery.min.js","hash":"82ab395176c927ffbb2f7c95132ee0a06cd5d64a","modified":1591422194166},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff","hash":"91daac2bfba5e6a1a15ce44c53eab524d01c7fb0","modified":1591422194141},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"f356aa2e4d9b7245985d312d3bfba180f774e3b7","modified":1591422194141},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"b2c74520c3f506efbfefca867918e5ae28bd5222","modified":1591422194155},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.ttf","hash":"f34ee061900ecd1ed3d3fd9f8f47f4e84c6d56bf","modified":1591422194141},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff","hash":"61f40daca6978e6e7ab761e748c2dd9d236c7586","modified":1591422194154},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.eot","hash":"d4987ee41e0e4142d561f76b8ea8e034c4d5d9d2","modified":1591422194135},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.svg","hash":"1622937e49766e21eacf4ac7065b711f0fe580e1","modified":1591422194143},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.eot","hash":"be6b63d528286b1be2328d871c9bae95d8d57174","modified":1591422194146},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-BoldItalic.ttf","hash":"b7d24ab1e4fad720f31a2b0cca1904ce1740d846","modified":1591422194183},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.ttf","hash":"91b40a8f284159d9fff81dc522670ef68d562682","modified":1591422194154},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Italic.ttf","hash":"9a23c6898b0943bd3d96c04df9a0f66e919451d8","modified":1591422194186},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Italic.ttf","hash":"93ebc5098cf57a32b7b8d297681f31692c09bdfa","modified":1591422194192},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Italic.ttf","hash":"9d757cc9f928fc83b2133283dd639c12b11d94ad","modified":1591422194198},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-BoldItalic.ttf","hash":"b542b9591fbf33925d93f0695b6e123a9f0cfd43","modified":1591422194191},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-BoldItalic.ttf","hash":"926035f0156cccf1b0ca507347f39bf9c510f51e","modified":1591422194195},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Bold.ttf","hash":"58be4b7760e9a84daa81929d046f9a15c4fd1c1a","modified":1591422194189},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Bold.ttf","hash":"f9918fb93d6ab6850f5d38069a999c311af78816","modified":1591422194194},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Regular.ttf","hash":"20ce1fc7ae1254558ca044ae48283faaa58897e5","modified":1591422194193},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Regular.ttf","hash":"de559f8d70d5b1ab2810597bfd0b1b9506f3ef01","modified":1591422194199},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Regular.ttf","hash":"6c090d6bff3928fbf8a5f4104e58ed7f421aea7c","modified":1591422194188},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Bold.ttf","hash":"34f7db59f1d023294e69976aa20b7d52b86165a4","modified":1591422194181},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.svg","hash":"29e6c7e5a4d63d2c9563cd208456cb4f8a357868","modified":1591422194139},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.svg","hash":"3a090431fdec61a25ed69b9e6f35a6656bde1595","modified":1591422194152},{"_id":"public/about/index.html","hash":"b01be6a819f66a9755cbb0cc0d5684efe59eb20e","modified":1591479832721},{"_id":"public/2020/06/06/hello-world/index.html","hash":"655a70a361e8940861d54f998126c0903721f684","modified":1591426168159},{"_id":"public/index.html","hash":"1109770d8fc555a085ddd289caa047c70699361b","modified":1591479832721},{"_id":"public/archives/index.html","hash":"2e13f1e5da545ddf29d8aa79e2d1f584aa78320c","modified":1591479832721},{"_id":"public/archives/2020/index.html","hash":"9a1a4586512ddac912638b68c266b7a1c03b0106","modified":1591479832721},{"_id":"public/archives/2020/06/index.html","hash":"cbeb4f9b2fd970897928b309e0e76c3694694c49","modified":1591426168159},{"_id":"public/images/favicon.ico","hash":"189f9842bcb79a6f8f9e8445bc8bbd773443826b","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Black.woff","hash":"9e8d954c46eaad8b8234fa906e9a268ee354dced","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Black.woff2","hash":"f1c5d7523d21c2bf820d827c9d5df4184c3866dc","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Light.woff","hash":"cba77d3d16f7565f9ea79bd7657f4e00c7fe851f","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Light.woff2","hash":"e520d5f6bf7ea3c1e4f2aef2abbbc6a6f9b697cb","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Medium.woff","hash":"2e8e3c873e6d98acc3c10aa84997104b276e1e68","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Medium.woff2","hash":"976b7aa7c2c2c049c548a25b5914cfbda74b0453","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Thin.woff","hash":"f231111b3c778b7a5898ea88c0f150c0e72be468","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Thin.woff2","hash":"baa42f95b41411b9aeaa6c7594e5ccee10d42ac4","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Bold.woff","hash":"9376516725e502f4375e06cc4fa7d940e2c93251","modified":1591479832721},{"_id":"public/images/apple-touch-icon.png","hash":"57e2def34682655f41a0be2d083f16765ba7858b","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir.woff","hash":"c798391d624b9bb44497a87ffc4f7eb52042dceb","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Bold.woff2","hash":"8d4810991aa94f958aff20a9cd381caf338e3061","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.eot","hash":"3ad44eb5c276d1435408f253ca78da729a1aca90","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir.woff2","hash":"dc3c0ed67c9abb062b562e8553776f614d2946c2","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"830f8653e5f4a5331ac0b47c5701f65fe9f1bb32","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff","hash":"f3f0ea4847825806062a9b7a0f629671eb6b6408","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.ttf","hash":"114f35e6d9053caca2ef6d1e34fea3f87a59245b","modified":1591479832721},{"_id":"public/images/favicon-192x192.png","hash":"96e6fcbbb13a5914a6131391e210eb7dfd13d692","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Black.eot","hash":"603acd29416644e4b4fb8646abeada1865ba6869","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Black.ttf","hash":"23ee4a19421de9a0ca9dddc5435a8efe5bf28d87","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Bold.ttf","hash":"658c1da4f2a0124f6340058daa6873a86e6ba4fc","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Bold.eot","hash":"9ef82b07f3adad7d644c9c3a6d35a0c727bd64e8","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Light.ttf","hash":"b2372b67b9519fb4fa8e05de6c0ddae56845ff79","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Light.eot","hash":"32f51bf715663f5ca419e138617fc05f7055aef5","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Medium.ttf","hash":"31cdbcc7215d01c9dd2beb8a28f8b7a7de75b9f4","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Thin.eot","hash":"2c0ebb336dd012da8d575cae0ee4d048b65fe6e1","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Medium.eot","hash":"17be9f699c30f0384004b46e991db8ac38a9992e","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir-Thin.ttf","hash":"d53fcc2e2d6c9c77613afcd34058be1b75bb0fef","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir.ttf","hash":"c7a3f2f4d56c4c4ec69d395baf39b55198da0254","modified":1591479832721},{"_id":"public/lib/vazir-font/Vazir.eot","hash":"bd3a7cb9eb70d36b4cfba8c5a05d234aefeefe3c","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff","hash":"91daac2bfba5e6a1a15ce44c53eab524d01c7fb0","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"b2c74520c3f506efbfefca867918e5ae28bd5222","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"f356aa2e4d9b7245985d312d3bfba180f774e3b7","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff","hash":"61f40daca6978e6e7ab761e748c2dd9d236c7586","modified":1591479832721},{"_id":"public/css/rtl.css","hash":"9589fac02a34fd9084f805f801889028756bbb65","modified":1591479832721},{"_id":"public/css/style.css","hash":"ff29132c9da449c45fa65f3d8861ae63dfbad6bf","modified":1591479832721},{"_id":"public/js/main.js","hash":"584c5a69ac81a483a1c4377a2e2cf326c2795e7b","modified":1591479832721},{"_id":"public/lib/clipboard/clipboard.min.js","hash":"ee60ca5ba9401456105ef703a98092369b579c80","modified":1591479832721},{"_id":"public/js/search.js","hash":"62df6eeb66e002a600317a288e3f8fcd0e9c5492","modified":1591479832721},{"_id":"public/lib/vazir-font/font-face.css","hash":"2a95709b15ee45fc2328051038ceedddf83235bb","modified":1591479832721},{"_id":"public/lib/jquery/jquery.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1591479832721},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"703603273f5d5d52eb456d6385e1a68294fbd568","modified":1591479832721},{"_id":"public/lib/justified-gallery/js/jquery.justifiedGallery.min.js","hash":"82ab395176c927ffbb2f7c95132ee0a06cd5d64a","modified":1591479832721},{"_id":"public/lib/justified-gallery/css/justifiedGallery.min.css","hash":"92bb6e468a1db7fbd99ccb960e15e28572254263","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.ttf","hash":"f34ee061900ecd1ed3d3fd9f8f47f4e84c6d56bf","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.eot","hash":"d4987ee41e0e4142d561f76b8ea8e034c4d5d9d2","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.svg","hash":"1622937e49766e21eacf4ac7065b711f0fe580e1","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.ttf","hash":"91b40a8f284159d9fff81dc522670ef68d562682","modified":1591479832721},{"_id":"public/images/logo.png","hash":"0e3029251dfda26adee2761f71377297e8c26871","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.eot","hash":"be6b63d528286b1be2328d871c9bae95d8d57174","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGM-BoldItalic.ttf","hash":"b542b9591fbf33925d93f0695b6e123a9f0cfd43","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGS-BoldItalic.ttf","hash":"926035f0156cccf1b0ca507347f39bf9c510f51e","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGL-Italic.ttf","hash":"9a23c6898b0943bd3d96c04df9a0f66e919451d8","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGS-Italic.ttf","hash":"9d757cc9f928fc83b2133283dd639c12b11d94ad","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGM-Italic.ttf","hash":"93ebc5098cf57a32b7b8d297681f31692c09bdfa","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGL-BoldItalic.ttf","hash":"b7d24ab1e4fad720f31a2b0cca1904ce1740d846","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGS-Bold.ttf","hash":"f9918fb93d6ab6850f5d38069a999c311af78816","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGM-Bold.ttf","hash":"58be4b7760e9a84daa81929d046f9a15c4fd1c1a","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGL-Regular.ttf","hash":"6c090d6bff3928fbf8a5f4104e58ed7f421aea7c","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGL-Bold.ttf","hash":"34f7db59f1d023294e69976aa20b7d52b86165a4","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGM-Regular.ttf","hash":"20ce1fc7ae1254558ca044ae48283faaa58897e5","modified":1591479832721},{"_id":"public/lib/meslo-LG/MesloLGS-Regular.ttf","hash":"de559f8d70d5b1ab2810597bfd0b1b9506f3ef01","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.svg","hash":"29e6c7e5a4d63d2c9563cd208456cb4f8a357868","modified":1591479832721},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.svg","hash":"3a090431fdec61a25ed69b9e6f35a6656bde1595","modified":1591479832721},{"_id":"source/_posts/wanns.md","hash":"050de177abb296fbe35083ee67ae549fb6ff0526","modified":1591471636014},{"_id":"source/_posts/wanns/wanns-minimal-net.png","hash":"e40849949c8bc0f051296bb09fc8c4153a11bfcc","modified":1591471474530},{"_id":"source/_posts/wanns/wanns-neat-genome.png","hash":"c34f412e4ba205a6f04a3ef47f99f736d0c3b887","modified":1591471474549},{"_id":"source/_posts/wanns/wanns-results-ic.png","hash":"1f7499a425b88b0ad615b68a773d780570505e05","modified":1591471474531},{"_id":"source/_posts/wanns/wanns-neat-crossover.png","hash":"f596b6aa625922af651f8796ee64434bb141309c","modified":1591471474531},{"_id":"source/_posts/wanns/wanns-results-rl.png","hash":"feaca19dc07d10a34a28c200a2fc6a1249030143","modified":1591471474582},{"_id":"public/2020/03/09/wanns/index.html","hash":"77c5a1ad580ec6ce16fcf955110fcda332b7cf2b","modified":1591479832721},{"_id":"public/archives/2020/03/index.html","hash":"53a88a2e29679718bdda4ccedae11d8a1535141a","modified":1591479832721},{"_id":"public/tags/NAS/index.html","hash":"d93054766ea2a3f6bab045536c14bac78b438699","modified":1591479832721},{"_id":"public/tags/EvolutionaryAlgorithms/index.html","hash":"b9823119c5ba020d806909d8576ffdd91c2cea3c","modified":1591479832721},{"_id":"public/tags/RL/index.html","hash":"775c7c9bc9740f094d5ed7370b08bd67c88b89da","modified":1591479832721},{"_id":"public/tags/ImageClassification/index.html","hash":"153e7e9d59e366e93e8bed899d600c292ea38069","modified":1591479832721},{"_id":"public/tags/NeurIPS2019/index.html","hash":"1ac23b614c6194d060f1343799c93aebe11a07ce","modified":1591479832721},{"_id":"public/2020/03/09/wanns/wanns-results-ic.png","hash":"1f7499a425b88b0ad615b68a773d780570505e05","modified":1591479832721},{"_id":"public/2020/03/09/wanns/wanns-neat-crossover.png","hash":"f596b6aa625922af651f8796ee64434bb141309c","modified":1591479832721},{"_id":"public/2020/03/09/wanns/wanns-neat-genome.png","hash":"c34f412e4ba205a6f04a3ef47f99f736d0c3b887","modified":1591479832721},{"_id":"public/2020/03/09/wanns/wanns-minimal-net.png","hash":"e40849949c8bc0f051296bb09fc8c4153a11bfcc","modified":1591479832721},{"_id":"public/2020/03/09/wanns/wanns-results-rl.png","hash":"feaca19dc07d10a34a28c200a2fc6a1249030143","modified":1591479832721},{"_id":"source/_posts/2020-06-06-wanns.md","hash":"050de177abb296fbe35083ee67ae549fb6ff0526","modified":1591474160709},{"_id":"source/_posts/2020-06-06-wanns/wanns-minimal-net.png","hash":"e40849949c8bc0f051296bb09fc8c4153a11bfcc","modified":1591472257854},{"_id":"source/_posts/2020-06-06-wanns/wanns-neat-crossover.png","hash":"f596b6aa625922af651f8796ee64434bb141309c","modified":1591472257848},{"_id":"source/_posts/2020-06-06-wanns/wanns-neat-genome.png","hash":"c34f412e4ba205a6f04a3ef47f99f736d0c3b887","modified":1591472257856},{"_id":"source/_posts/2020-06-06-wanns/wanns-results-ic.png","hash":"1f7499a425b88b0ad615b68a773d780570505e05","modified":1591472257855},{"_id":"source/_posts/2020-06-06-wanns/wanns-results-rl.png","hash":"feaca19dc07d10a34a28c200a2fc6a1249030143","modified":1591472257878},{"_id":"source/_drafts/lifelongrl.md","hash":"d51b4e89b93dbf68dc96ce6d9b54b1a58f494e33","modified":1591474004558},{"_id":"source/_drafts/lifelongrl/RL.png","hash":"8a8c0a3463edd57bd2c931303cc1880ffe60b4f9","modified":1591473970895},{"_id":"public/2020/06/06/lifelongrl/RL.png","hash":"8a8c0a3463edd57bd2c931303cc1880ffe60b4f9","modified":1591479832721}],"Category":[{"name":"NAS","_id":"ckb40oyg60001n1592z3s8mex"},{"name":"EvolutionaryAlgorithms","parent":"ckb40oyg60001n1592z3s8mex","_id":"ckb40oyg90002n159e2zw6n43"},{"name":"RL","parent":"ckb40oyg90002n159e2zw6n43","_id":"ckb40oyg90003n1595vfdg67h"},{"name":"ImageClassification","parent":"ckb40oyg90003n1595vfdg67h","_id":"ckb40oyga0004n159ffpaesqd"},{"name":"NeurIPS2019","parent":"ckb40oyga0004n159ffpaesqd","_id":"ckb40oygb0005n159aog9cww7"}],"Data":[],"Page":[{"title":"about","date":"2020-06-06T05:45:22.000Z","_content":"\n# About the founder\n\nHi! My name is Jorge. My degree says I am a Data Scientist, but I prefer to see myself as a Machine Learning Engineer (a.k.a. Data Science Engineer) because I really cannot see Data Science in the same way that traditional job descriptions describe it.\n\n# The short story\n\n\n\nThe _ML summaries_ started in early 2020. My motivation was very simple: help others to understand Machine Learning (ML) best papers without struggling too much, ideally in just on read!\n\n# A more elaborated story\n\nWell, you migh say that is a clich in the world of ML blogs. If you think so, I must admit you are right... But I like to believe there is something that makes the _ML summaries_ different from other blogs: my formation when I entered the ML ecosystem was not that of a data scientist, nor a mathematician, nor a physician, and **I really really struggled getting there!** I was a computer scientist that finished his bachelor when machine learning was just starting and it was rarely introduced in most of the universities. I mean, when I did my bachelor around 2010, the main aspiration of us, the average computer scientists, was to become a web developer, master hibernate or spring, learn everything about agile methodologies, and all that amazing stuff (I genuinely believe that) that was the interest back then and which made the web such a comfortable place. So, even if we had good mathematical backgrounds in college, or if we were lucky enough to had been introduced to (now) old machine learning models, it was not something you really used while learning Java EE. The job market needed developers and that is what we became.\n\nSo, when I decided to pursue a master in Data Science in 2017  5 years after my last mathematics course  I struggled understanding all those groundbreaking papers that talked about softmax, backpropagation, cross-entropy, bayesian learning, LSTMs, reinforcement learning, and uncountable others. I struggled remembering my basic statistics, and understanding more elaborated ones that I had never seen. I hardly remembered what the gradient or the central limit theorem were. Plus, after sometime I realized I was not the only one struggling with that. In particular, every single person that was educated before the boom of Machine Learning was struggling in similar ways.\n\nDuring that journey I realized something: thinking as a developer eventually helped me to structure my new ML knowledge a bit better, and also made a difference in my ML contributions. I believe that a great skill that software engineers have is tha ability to think in boxes. The ML summaries is born with that idea in mind, so that people with a similar background can have a new perspective of how ML can be understood too. Perhaps, for some ML practitioners writting in mathematical terms is straightforward to understand, but some of us prefer to understand ideas in a different way because the other way is not as straightforward. Sometimes we need the author to remember us the basic ideas that might not be as clear now. Sometimes, we would love to have a better starting point to understand an idea that would take weeks to grasp if were to do it alone. And the box thinking come really handy in that!\n\nBut enough talking! I hope that this explanation helps to understand the key idea of the _ML summaries_ blog. If you feel my story relatable, then you could also join me in this quest!\n\nI hope you enjoy the summaries, and good luck in your ML journey!\n\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2020-06-06 00:45:22\n---\n\n# About the founder\n\nHi! My name is Jorge. My degree says I am a Data Scientist, but I prefer to see myself as a Machine Learning Engineer (a.k.a. Data Science Engineer) because I really cannot see Data Science in the same way that traditional job descriptions describe it.\n\n# The short story\n\n\n\nThe _ML summaries_ started in early 2020. My motivation was very simple: help others to understand Machine Learning (ML) best papers without struggling too much, ideally in just on read!\n\n# A more elaborated story\n\nWell, you migh say that is a clich in the world of ML blogs. If you think so, I must admit you are right... But I like to believe there is something that makes the _ML summaries_ different from other blogs: my formation when I entered the ML ecosystem was not that of a data scientist, nor a mathematician, nor a physician, and **I really really struggled getting there!** I was a computer scientist that finished his bachelor when machine learning was just starting and it was rarely introduced in most of the universities. I mean, when I did my bachelor around 2010, the main aspiration of us, the average computer scientists, was to become a web developer, master hibernate or spring, learn everything about agile methodologies, and all that amazing stuff (I genuinely believe that) that was the interest back then and which made the web such a comfortable place. So, even if we had good mathematical backgrounds in college, or if we were lucky enough to had been introduced to (now) old machine learning models, it was not something you really used while learning Java EE. The job market needed developers and that is what we became.\n\nSo, when I decided to pursue a master in Data Science in 2017  5 years after my last mathematics course  I struggled understanding all those groundbreaking papers that talked about softmax, backpropagation, cross-entropy, bayesian learning, LSTMs, reinforcement learning, and uncountable others. I struggled remembering my basic statistics, and understanding more elaborated ones that I had never seen. I hardly remembered what the gradient or the central limit theorem were. Plus, after sometime I realized I was not the only one struggling with that. In particular, every single person that was educated before the boom of Machine Learning was struggling in similar ways.\n\nDuring that journey I realized something: thinking as a developer eventually helped me to structure my new ML knowledge a bit better, and also made a difference in my ML contributions. I believe that a great skill that software engineers have is tha ability to think in boxes. The ML summaries is born with that idea in mind, so that people with a similar background can have a new perspective of how ML can be understood too. Perhaps, for some ML practitioners writting in mathematical terms is straightforward to understand, but some of us prefer to understand ideas in a different way because the other way is not as straightforward. Sometimes we need the author to remember us the basic ideas that might not be as clear now. Sometimes, we would love to have a better starting point to understand an idea that would take weeks to grasp if were to do it alone. And the box thinking come really handy in that!\n\nBut enough talking! I hope that this explanation helps to understand the key idea of the _ML summaries_ blog. If you feel my story relatable, then you could also join me in this quest!\n\nI hope you enjoy the summaries, and good luck in your ML journey!\n\n","updated":"2020-06-06T21:28:07.999Z","path":"about/index.html","_id":"ckb37t6fq0001b359bk0w95pb","comments":1,"layout":"page","content":"<h1 id=\"About-the-founder\"><a href=\"#About-the-founder\" class=\"headerlink\" title=\"About the founder\"></a>About the founder</h1><p>Hi! My name is Jorge. My degree says I am a Data Scientist, but I prefer to see myself as a Machine Learning Engineer (a.k.a. Data Science Engineer) because I really cannot see Data Science in the same way that traditional job descriptions describe it.</p>\n<h1 id=\"The-short-story\"><a href=\"#The-short-story\" class=\"headerlink\" title=\"The short story\"></a>The short story</h1><p>The <em>ML summaries</em> started in early 2020. My motivation was very simple: help others to understand Machine Learning (ML) best papers without struggling too much, ideally in just on read!</p>\n<h1 id=\"A-more-elaborated-story\"><a href=\"#A-more-elaborated-story\" class=\"headerlink\" title=\"A more elaborated story\"></a>A more elaborated story</h1><p>Well, you migh say that is a clich in the world of ML blogs. If you think so, I must admit you are right But I like to believe there is something that makes the <em>ML summaries</em> different from other blogs: my formation when I entered the ML ecosystem was not that of a data scientist, nor a mathematician, nor a physician, and <strong>I really really struggled getting there!</strong> I was a computer scientist that finished his bachelor when machine learning was just starting and it was rarely introduced in most of the universities. I mean, when I did my bachelor around 2010, the main aspiration of us, the average computer scientists, was to become a web developer, master hibernate or spring, learn everything about agile methodologies, and all that amazing stuff (I genuinely believe that) that was the interest back then and which made the web such a comfortable place. So, even if we had good mathematical backgrounds in college, or if we were lucky enough to had been introduced to (now) old machine learning models, it was not something you really used while learning Java EE. The job market needed developers and that is what we became.</p>\n<p>So, when I decided to pursue a master in Data Science in 2017  5 years after my last mathematics course  I struggled understanding all those groundbreaking papers that talked about softmax, backpropagation, cross-entropy, bayesian learning, LSTMs, reinforcement learning, and uncountable others. I struggled remembering my basic statistics, and understanding more elaborated ones that I had never seen. I hardly remembered what the gradient or the central limit theorem were. Plus, after sometime I realized I was not the only one struggling with that. In particular, every single person that was educated before the boom of Machine Learning was struggling in similar ways.</p>\n<p>During that journey I realized something: thinking as a developer eventually helped me to structure my new ML knowledge a bit better, and also made a difference in my ML contributions. I believe that a great skill that software engineers have is tha ability to think in boxes. The ML summaries is born with that idea in mind, so that people with a similar background can have a new perspective of how ML can be understood too. Perhaps, for some ML practitioners writting in mathematical terms is straightforward to understand, but some of us prefer to understand ideas in a different way because the other way is not as straightforward. Sometimes we need the author to remember us the basic ideas that might not be as clear now. Sometimes, we would love to have a better starting point to understand an idea that would take weeks to grasp if were to do it alone. And the box thinking come really handy in that!</p>\n<p>But enough talking! I hope that this explanation helps to understand the key idea of the <em>ML summaries</em> blog. If you feel my story relatable, then you could also join me in this quest!</p>\n<p>I hope you enjoy the summaries, and good luck in your ML journey!</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"About-the-founder\"><a href=\"#About-the-founder\" class=\"headerlink\" title=\"About the founder\"></a>About the founder</h1><p>Hi! My name is Jorge. My degree says I am a Data Scientist, but I prefer to see myself as a Machine Learning Engineer (a.k.a. Data Science Engineer) because I really cannot see Data Science in the same way that traditional job descriptions describe it.</p>\n<h1 id=\"The-short-story\"><a href=\"#The-short-story\" class=\"headerlink\" title=\"The short story\"></a>The short story</h1><p>The <em>ML summaries</em> started in early 2020. My motivation was very simple: help others to understand Machine Learning (ML) best papers without struggling too much, ideally in just on read!</p>\n<h1 id=\"A-more-elaborated-story\"><a href=\"#A-more-elaborated-story\" class=\"headerlink\" title=\"A more elaborated story\"></a>A more elaborated story</h1><p>Well, you migh say that is a clich in the world of ML blogs. If you think so, I must admit you are right But I like to believe there is something that makes the <em>ML summaries</em> different from other blogs: my formation when I entered the ML ecosystem was not that of a data scientist, nor a mathematician, nor a physician, and <strong>I really really struggled getting there!</strong> I was a computer scientist that finished his bachelor when machine learning was just starting and it was rarely introduced in most of the universities. I mean, when I did my bachelor around 2010, the main aspiration of us, the average computer scientists, was to become a web developer, master hibernate or spring, learn everything about agile methodologies, and all that amazing stuff (I genuinely believe that) that was the interest back then and which made the web such a comfortable place. So, even if we had good mathematical backgrounds in college, or if we were lucky enough to had been introduced to (now) old machine learning models, it was not something you really used while learning Java EE. The job market needed developers and that is what we became.</p>\n<p>So, when I decided to pursue a master in Data Science in 2017  5 years after my last mathematics course  I struggled understanding all those groundbreaking papers that talked about softmax, backpropagation, cross-entropy, bayesian learning, LSTMs, reinforcement learning, and uncountable others. I struggled remembering my basic statistics, and understanding more elaborated ones that I had never seen. I hardly remembered what the gradient or the central limit theorem were. Plus, after sometime I realized I was not the only one struggling with that. In particular, every single person that was educated before the boom of Machine Learning was struggling in similar ways.</p>\n<p>During that journey I realized something: thinking as a developer eventually helped me to structure my new ML knowledge a bit better, and also made a difference in my ML contributions. I believe that a great skill that software engineers have is tha ability to think in boxes. The ML summaries is born with that idea in mind, so that people with a similar background can have a new perspective of how ML can be understood too. Perhaps, for some ML practitioners writting in mathematical terms is straightforward to understand, but some of us prefer to understand ideas in a different way because the other way is not as straightforward. Sometimes we need the author to remember us the basic ideas that might not be as clear now. Sometimes, we would love to have a better starting point to understand an idea that would take weeks to grasp if were to do it alone. And the box thinking come really handy in that!</p>\n<p>But enough talking! I hope that this explanation helps to understand the key idea of the <em>ML summaries</em> blog. If you feel my story relatable, then you could also join me in this quest!</p>\n<p>I hope you enjoy the summaries, and good luck in your ML journey!</p>\n"}],"Post":[{"title":"Weight Agnostic Neural Networks (Gaier and Ha)","date":"2020-03-09T17:00:00.000Z","updated":"2020-06-05T17:14:11.000Z","_content":"\n## Introduction\n\nI got aware of this paper at the NeurIPS 2019 conference in Vancouver. You can find the official paper <a href=\"https://arxiv.org/abs/1906.04358\" target=\"_blank\">here</a>, and an interactive version <a href=\"https://weightagnostic.github.io\" target=\"_blank\">at this link</a>. The source code is hosted in <a href=\"https://github.com/google/brain-tokyo-workshop/tree/master/WANNRelease\" target=\"_blank\">this GitHub repo</a>  although it is a bit hard to get through it because of the directory structure.\n\nThe main highlight of the paper is that _artificially crafted neural networks with a single weight value  shared by all neurons and with no training  achieve competitive performance on popular baselines_. Such networks are labeled as Weight-Agnostic Neural Networks (WANNs). Before going through the paper, I recommend checking some basic concepts about simple neural network architectures, evolutionary algorithms, image classification, and reinforcement learning.\n\n## Background\n\n### Neural Architecture Search (NAS)\n\nIn a nutshell, the ultimate goal of NAS is to get rid of humans when selecting/designing the best architecture for a dataset. It does so by creating algorithms to automatically select the best arrangement of layers and the best hyperparameters. We can find a good number of papers dealing with this, and every approach has its strengths and weaknesses, but perhaps the one weakness they all share is the expensiveness of the training and evaluation of the neural networks. Why is this a problem? Because in a search process, we got to compare a significant number of neural architectures to see which ones perform best, based on its train/test accuracy. If we were to compare, let's say 10, it would not be a problem if each of the networks took 2 minutes to train, but if we compare thousands of architectures then it starts being a problem.\n\n> If you want to read more about NAS, I recommend the <a href=\"https://arxiv.org/abs/1808.05377\" target=\"_blank\">survey by Elsken et al.</a>\n\n### Evolutionary Algorithms (EAs)\n\nAn EA is an optimization method inspired by biological evolution. In short, it claims that, for a given problem, you can start with a set of random solutions (a population), which will be altered with some operations (crossover and mutation) so that they can \"evolve\" into a better solution, similar to what nature does with species.\n\n<a href=\"https://en.wikipedia.org/wiki/Evolutionary_algorithm#Implementation\" target=\"_blank\">Wikipedia's pseudo-code</a> summarizes the generic EA steps as follow:\n\n1. Generate the initial population of organisms randomly (**first generation**).\n2. **Repeat** the following **regenerational steps** until termination:\n   1. **Evaluate** the **fitness of each organism** in the population (i.e., a measure of their quality).\n   2. **Select** the **fittest** organisms **for reproduction**.\n   3. **Breed new organisms through crossover and mutation** operations to give birth to offspring.\n   4. **Evaluate** the fitness of **new organisms**.\n   5. **Replace** the **least-fit** organisms of the population **with new organisms**.\n\nThe loop of step 2 can variate in the order of the sub-steps, but this reflects the general idea.\n\n### NeuroEvolution of Augmenting Topologies (NEAT)\n\n<a href=\"http://nn.cs.utexas.edu/keyword?stanley:ec02\" blank=\"_target\">NEAT (2002)</a> is an EA approach to NAS. The algorithm for Weight-Agnostic Neural Networks is highly inspired by it, thus it is convenient to take a quick look at it.\n\nNEAT encodes the networks (i.e., the organisms) into _genomes_, as shown in Figure 1. A _genome_ contains a list of _connection genes_ which capture the topology of the network (i.e., the architecture). Each element in the list has 4 main attributes: `in` node, `out` node, `weight`, and `innovation number` (a counter of the latest generation in which the node was mutated). The remaining attribute (`enabled`) will help during the mutations because connections can change.\n\n{% asset_img wanns-neat-genome.png \"Figure 1: Example of a Genome in NEAT\" %}\n\n<br/>\nNEAT's goal is to give birth to the best possible network with the next pseudo-code:\n\n1. Generate **initial population**.\n2. **Repeat** `m` times:\n   1. **Compute the fitness** of the organisms.\n   2. Generate **offspring** according to fitness.\n   3. **Replace the population** with the offspring.\n\n\nLet's understand each of the key elements.\n\n**The initial population**. NEAT considers a _uniform_ first generation of _minimal networks_. A _minimal network_ is one with only inputs and outputs (i.e., no hidden nodes). _Uniform_ seems to mean that all the organisms are the same (fully-connected), and just the weights differ. The initial population is of size `N` and is kept constant through all iterations.\n\n**Evaluation of fitness**. To evaluate the fitness of the organisms, NEAT clusters the population into _species_ (similar topologies are put together) so that members of the same species will have similar fitness values. Given two organisms `x` and `y`, NEAT proposes a function ` = (x, y)` that measures how similar the organisms are. Hence, with a so-called _compatibility threshold_ `_t`, it is possible to cluster the population by comparing ` < _t`. The fitness `f_x` of the organism `x`, belonging to species `s_i`, is a function of `s_i` and the reward `r_x` of the network: `f_x=f(s_i, r_x)`. The reward can be the accuracy of the network or any other measure of its performance in a given task. To see the actual functions `` and `f_x`, consult page 13 of the <a href=\"http://nn.cs.utexas.edu/keyword?stanley:ec02\" blank=\"_target\">NEAT paper</a>. \n\n**Generating the offspring**. In each generation, the `N` organisms will be replaced by the offspring. The offspring is _per species_. 25% of the offspring (`A = 0.25 * N`) results from mutation without crossover, and (presumably) 75% (`B = 0.75 * N`) from reproduction (crossover + posterior mutation). The first step for reproduction is to eliminate the lowest-performing organisms from the entire population (the \"lowest\" criteria is not mentioned in the paper, sorry). Then, for each species `s_i`, the within-species fitness sum is computed: `F_i = sum_all(f_x)` for all `x` in `s_i`. Next, we require the proportion of `F_i` with respect to all species: `o_i = F_i / sum_all(F_j)`. Each species `s_i` gets assigned a number `o_i * B` of offspring. It is mating time now!\n\nThe specifics of mutation and crossover are as follow:\n\n- **Mutation**: There are three mutation operators: _weight perturbation_, _add connection_, and _add node_. I am skipping the first one because we do not need it for WANNs. <u>Add connection</u> selects two unconnected nodes (presumably at random) and links them with a new random weight `w`. <u>Add node</u> selects an existing connection (presumably at random too) with some weight `w_c` and splits it in two, placing a new node in between. The new weights are `w_in = 1` and `w_out = w_c`. The old connection is set to `DISABLED`.\n- **Crossover**: When crossing over, genomes `x` and `y` are lined up as shown in Figure 2. If two genes `x[i]` and `y[i]` have the same `innovation number` they are called _matching genes_. If they are not matching, then they are either _disjoint_ (if the `innovation number` of `x[i]` <= `y[i]`, or vice versa) or _excess_ (if the `innovation number` of `x[i]` > `y[i]`, or vice versa). At the time of selecting the genes for the offspring, _matching genes_ are selected at random from either parent and _excess_/_disjoint_ are taken from the most-fit parent (if they are equally fit then they are chosen randomly).\n\n{% asset_img wanns-neat-crossover.png \"Figure 2. Crossover in NEAT. The top numbers represent the `innovation number`\" %}\n\n<br/>\nThere are some specifics about the NEAT implementation that you can consult on pages 14 and 15 (section: \"Performance Evaluations\") of the paper. But roughly, this is what you need to understand WANNs.\n\n### Dominance relations\n\n_Dominance_ is a concept from <a href=\"https://en.wikipedia.org/wiki/Multi-objective_optimization\" target=\"_blank\">multi-objective optimization</a>, where we want to find the optimal value (minimum or maximum) for a set of functions. That is, `min( f_1(x), f_2(x), ... , f_k(x) )`, where `k` is the number of objectives. `x* in X` is called a _feasible solution_, and `X` is the _feasible set_ of decision vectors. A vector `z* := < f_1(x*), f_2(x*), ... f_k(x*)>` for a feasible solution `x*` is called an _outcome_.\n\nA feasible solution `x1` is said to (Pareto) **dominate** a solution `x2` (for minimization) if:\n1. `f_i(x1) <= f_i(x_2)` **for all** indices `i in {1, 2, ..., k}`\n2. `f_j(x1) < f_i(x_2)` **for at least one** index `j in {1, 2, ..., k}`\n\nIn words, `x1` dominates `x2` if `x2` does not improve any of the objectives `f_i`  when compared to `x1`, and there is at least one `f_i` for which `x1` is strictly better than `x2`.\n\nA solution `x*` is called _Pareto optimal_ (a.k.a. non-dominated or Pareto efficient) if there does not exist another solution that dominates it. The set `F` of Pareto optimal solutions is called _Pareto front_.\n\nIt is possible to group the solutions `x_i` in fronts. First, using the set `X` we need to compute the first front `F_1`. Second, we compute `X = X - F1`  and compute a new front `F_2`. We can repeat this process until `X` is empty. An efficient algorithm for this process is on <a href=\"http://www.dmi.unict.it/mpavone/nc-cs/materiale/NSGA-II.pdf\" target=\"_blank\">Page 3 of this paper</a> cited by Gaier and Ha.\n\n## The approach in a nutshell\n\nGaier and Ha propose a search algorithm where the constructed **networks will not be trained**, saving us a lot of computational resources. The core of the algorithm is [NEAT](#neuroevolution-of-augmenting-topologies-neat) with some minor modifications:\n\n1. The **initial population** is **not uniform**.\n2. **For every architecture a weight** from a set `W` **is used**. **All neurons** in the architecture  **share this weight** and the network will infer with no training whatsoever. This is repeated for each weight in `W` (each trial is called a _rollout_).\n3. The **fitness considers performance** (without training) over the rollouts **and complexity** of the networks.\n4. A **modified mutation operator**.\n\nThe algorithm was tested for reinforcement learning and image classification tasks. We just need to elaborate on each of the modifications before looking at the main results.\n\n## Understanding the key elements\n\n### The initial set of architectures\n\nSimilarly to NEAT, the algorithm starts with _minimal topologies_. By looking at an example in the WANNs' paper (see Figure 3) it is possible to infer a small modification on what a minimal topology is. Instead of fully connected inputs/outputs, WANNs start from **a population of networks with zero hidden nodes, which have inputs _randomly_ connected to outputs**.\n\n{% asset_img wanns-minimal-net.png \"Figure 3: Example of a minimal topology\" %}\n\n### The rollouts\n\nThe rollouts are the key aspect of how to measure the performance of the architectures. _A rollout_ is the evaluation of a network with one shared weight. The weights considered are in the set `W = [-2, -1, -0.5, +0.5, +1, +2]`. For each weight in `W`, the network is evaluated. For RL that means obtaining a cumulative reward, and for image classification the accuracy.\n\nAfter all evaluations, three measures are computed:\n1. `f_1`: The average of the rollouts' rewards/accuracies.\n2. `f_2`: The maximum reward/accuracy among rollouts.\n3. `f_3`: The number of connections in the architecture.\n\n### The fitness values\n\nOnce the rollouts are finished, the **fitness** is measured. The authors propose a stochastic approach:\n\n- 20% of the time, the fitness is the rank after sorting by _dominance relations_ of the objectives `f_1` and `f_2`.\n- 80% of the time, the fitness is the rank after sorting by _dominance relations_ of the objectives `f_1` and `f_3`.\n\n### The mutation operators\n\nThe architectures mutate via **three operators**:\n\n- **Insert new node**: Same as in NEAT, but without weights and with an activation function sampled at random from the set `[linear, step, sin, cosine, Gaussian, tanh, sigmoid, absolute-value, negative linear, ReLU]`.\n- **Add new connection**: Same as in NEAT, but without weights.\n- **Change existing activation**: An existing node is chosen (presumably at random). Then, an activation function `F` is randomly selected from the set `[linear, step, sin, cosine, Gaussian, tanh, sigmoid, absolute-value, negative linear, ReLU]` and the node gets `F` assigned.\n\n## Experiments and results\n\nAs mentioned earlier, WANNs are tested on reinforcement learning (RL) and image classification tasks. To avoid rephrasing the paper, I limit myself to present an overview of the findings, emphasizing what I consider the most interesting. I recommend checking all the results in the <a href=\"https://weightagnostic.github.io\" target=\"_blank\">paper's interactive version</a>.\n\n### Reinforcement Learning\n\n#### Experiment setup\n\nThe reinforcement learning tasks solved with WANNs are `CartPoleSwingUp`, `BipedalWalker-v2`, and `CarRacing-v0` (all of them available as <a href=\"https://gym.openai.com/envs/\" target=\"_blank\">OpenAI Gym environments</a>). For each of the tasks, a WANN is constructed by following the approach we have discussed so far. To compare the WANN's performance, the authors use a <a href=\"https://worldmodels.github.io\" target=\"_blank\">fixed topology from literatue</a> as a baseline. For the two architectures, the next experiments are repeated 100 times per task:\n\n1. Solve the task with random weights (drawn from `Uniform(-2, 2)`) assigned per neuron.\n2. Solve the task after training (tuning) of random weights (drawn from `Uniform(-2, 2)`) assigned per neuron.\n3. Solve the task with a shared random weight (drawn from `Uniform(-2, 2)`).\n4. Solve the task after training (tuning) of a shared random weight (drawn from `Uniform(-2, 2)`).\n\nThis setting makes a lot of sense to me. By comparing the performance using non-tuned random weights, we can see how dependent an architecture is on the weights. If it performs poorly, then the architecture might not be adequate for the task and relies too much on the weights. On the other hand, when tuning the weights, important things can be revealed: 1) if the architecture performed poorly with random weights but improved with tuning, then it could suggest that the tuned weights _hide_ the fact that the architecture is not adequate by design; 2) if a WANN does not improve much when tuned, the topology itself might be \"overfitting\" (which is not necessarily a bad thing because then we could forget about _general-purpose_ architectures and start designing _ad hoc_ topologies for our tasks).\n\n#### Results\n\nThe reported rewards are shown in Figure 4. In short, WANNs outperform the baseline in 11/12 experiments. In general, the baseline appears very dependent on the tuned weights (very low or even negative rewards when randomly assigned). A WANN has the same drawback for random weights but it is way more robust to a random shared weight. \n\nFor the rest of the analysis let's forget about the baseline. A WANN achieves its maximum performance after tuning of non-shared weights, which also improves its variance. The reward obtained with random shared weights is still low compared to the maximum obtained and shows important variance. When the shared weight is tuned, the performance becomes more certain and importantly higher but not the highest.\n\n{% asset_img wanns-results-rl.png \"Figure 4: Results for the RL experiments, as reported in the original paper.\" %}\n\n<br/>\nWith these numbers, it is not clear yet what the essential advantage of a WANN is. To clear this out, Gaier and Ha help us with an analysis of the topologies discovered, from which I will simply rephrase the main claims. WANNs are simple and modular. They seem to ignore non-important inputs and use fewer connections than state-of-the-art approaches. An important note made in the paper is that the complexity of the topologies increases at each generation, capturing one by one the principal elements of the task. For example, in the `CartPoleSwingUp` task, a topology from an early generation captures the position-velocity relation, and in a later generation, the same topology adds nodes on top that refine the balancing (check the <a href=\"https://weightagnostic.github.io\" target=\"_blank\">interactive paper</a> for an animation of this behavior).\n\n### Image Classification\n\n#### Experiment setup\n\nFor Image Classification the setup changes. The task is now <a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">MNIST</a>. This time the baselines are a simple Linear Regressor and a Two-Layer CNN. Presumably, the next steps are followed 100 times too: \n\n1. Solve the task with a shared random weight (drawn from `Uniform(-2, 2)`).\n2. Solve the task after training (tuning) of a shared random weight (drawn from `Uniform(-2, 2)`).\n3. Instantiate the WNN with `m` different weights (without training) so that we end up with `m` WNNs with the same topology and just the shared weight differing. Build an ensemble where each of these networks is given a single vote so that the predicted value for an image is the digit that received the most votes.\n4. Repeat 3, but training each member of the ensemble (I assume) individually.\n\nA strong justification for trying with an ensemble in this task is not clear to me. My impression is that since the results did not show as strong as in RL, they experimented with an alternative that could strengthen WANNs. The task itself is simple to me, considering that a simple Two-Layer CNN is already a strong baseline for MNIST.\n\n#### Results\n\nThe results are in Figure 5. This time WANNs do not seem as well-performing as in the RL experiment. As I said, maybe that's the reason behind trying with an ensemble. I do not have much to say about this experiment, just that it seems to me that there is much room for improvement in the experiment setup. \n\n{% asset_img wanns-results-ic.png \"Figure 5: Results for the Image Classification experiments, as reported in the original paper.\" %}\n\n## My feelings about the paper\n\nI really like this paper. From the moment I knew about it, I was impressed for its simplicity and power. We are always worried about the best weights of a network and put too much care in their training to compare the architectures, but this work shows that a single shared weight can help us to compare and build architectures efficiently.\n\nMy main takeover is that a WANN has a more adequate topology because forgetting about the weights and training makes the algorithm to focus on the arrangement of layers. However, I am a bit worried about the variance of the random shared weight (Figure 4). Also, I am not clear on the selection of the baseline for the RL tasks. Are they convenient? Are there better baselines? I must admit I did not read the baseline's paper. If I have the time to do it, I will update this part of the summary with more insights.\n\nI like the fitness function, wich uses dominance relations. By considering different objectives, the estimation of the performance looks more solid to me. \n\nConcerning the image classification experiment, I have to say that I was expecting a bit more of discussion but it is great that Gaier and Ha spent time to show us WANNs in that domain too. To me, WANNs are not looking so powerful there, but it is also true that researchers have made much progress in image classification and sometimes is unfair for the novel approaches to compete with state-of-the-art architectures in the field, even though they have excellent ideas as it is the case of Gaier and Ha.\n\nIn general, I think it would be good to see how other search methods (e.g. RL) behave with WANNs, specially because of the recent success of NAS with RL. I would also like to see the running times, because although the search does not look expensive, solving the RL environments could be. As it is the case with some papers from the Big Guys, the results look amazing but the computational resources could be out of our reach.\n\nThis is all for my summary. I hope this helps you to understand and possibly implement the paper. Do not forget to share :)\n","source":"_posts/2020-06-06-wanns.md","raw":"---\ntitle:  Weight Agnostic Neural Networks (Gaier and Ha)\ndate:   2020/03/10 00:00:00 +0700\nupdated: 2020-06-06 00:14:11 +0700\ntags: [NAS, EvolutionaryAlgorithms, RL, ImageClassification, NeurIPS2019]\n---\n\n## Introduction\n\nI got aware of this paper at the NeurIPS 2019 conference in Vancouver. You can find the official paper <a href=\"https://arxiv.org/abs/1906.04358\" target=\"_blank\">here</a>, and an interactive version <a href=\"https://weightagnostic.github.io\" target=\"_blank\">at this link</a>. The source code is hosted in <a href=\"https://github.com/google/brain-tokyo-workshop/tree/master/WANNRelease\" target=\"_blank\">this GitHub repo</a>  although it is a bit hard to get through it because of the directory structure.\n\nThe main highlight of the paper is that _artificially crafted neural networks with a single weight value  shared by all neurons and with no training  achieve competitive performance on popular baselines_. Such networks are labeled as Weight-Agnostic Neural Networks (WANNs). Before going through the paper, I recommend checking some basic concepts about simple neural network architectures, evolutionary algorithms, image classification, and reinforcement learning.\n\n## Background\n\n### Neural Architecture Search (NAS)\n\nIn a nutshell, the ultimate goal of NAS is to get rid of humans when selecting/designing the best architecture for a dataset. It does so by creating algorithms to automatically select the best arrangement of layers and the best hyperparameters. We can find a good number of papers dealing with this, and every approach has its strengths and weaknesses, but perhaps the one weakness they all share is the expensiveness of the training and evaluation of the neural networks. Why is this a problem? Because in a search process, we got to compare a significant number of neural architectures to see which ones perform best, based on its train/test accuracy. If we were to compare, let's say 10, it would not be a problem if each of the networks took 2 minutes to train, but if we compare thousands of architectures then it starts being a problem.\n\n> If you want to read more about NAS, I recommend the <a href=\"https://arxiv.org/abs/1808.05377\" target=\"_blank\">survey by Elsken et al.</a>\n\n### Evolutionary Algorithms (EAs)\n\nAn EA is an optimization method inspired by biological evolution. In short, it claims that, for a given problem, you can start with a set of random solutions (a population), which will be altered with some operations (crossover and mutation) so that they can \"evolve\" into a better solution, similar to what nature does with species.\n\n<a href=\"https://en.wikipedia.org/wiki/Evolutionary_algorithm#Implementation\" target=\"_blank\">Wikipedia's pseudo-code</a> summarizes the generic EA steps as follow:\n\n1. Generate the initial population of organisms randomly (**first generation**).\n2. **Repeat** the following **regenerational steps** until termination:\n   1. **Evaluate** the **fitness of each organism** in the population (i.e., a measure of their quality).\n   2. **Select** the **fittest** organisms **for reproduction**.\n   3. **Breed new organisms through crossover and mutation** operations to give birth to offspring.\n   4. **Evaluate** the fitness of **new organisms**.\n   5. **Replace** the **least-fit** organisms of the population **with new organisms**.\n\nThe loop of step 2 can variate in the order of the sub-steps, but this reflects the general idea.\n\n### NeuroEvolution of Augmenting Topologies (NEAT)\n\n<a href=\"http://nn.cs.utexas.edu/keyword?stanley:ec02\" blank=\"_target\">NEAT (2002)</a> is an EA approach to NAS. The algorithm for Weight-Agnostic Neural Networks is highly inspired by it, thus it is convenient to take a quick look at it.\n\nNEAT encodes the networks (i.e., the organisms) into _genomes_, as shown in Figure 1. A _genome_ contains a list of _connection genes_ which capture the topology of the network (i.e., the architecture). Each element in the list has 4 main attributes: `in` node, `out` node, `weight`, and `innovation number` (a counter of the latest generation in which the node was mutated). The remaining attribute (`enabled`) will help during the mutations because connections can change.\n\n{% asset_img wanns-neat-genome.png \"Figure 1: Example of a Genome in NEAT\" %}\n\n<br/>\nNEAT's goal is to give birth to the best possible network with the next pseudo-code:\n\n1. Generate **initial population**.\n2. **Repeat** `m` times:\n   1. **Compute the fitness** of the organisms.\n   2. Generate **offspring** according to fitness.\n   3. **Replace the population** with the offspring.\n\n\nLet's understand each of the key elements.\n\n**The initial population**. NEAT considers a _uniform_ first generation of _minimal networks_. A _minimal network_ is one with only inputs and outputs (i.e., no hidden nodes). _Uniform_ seems to mean that all the organisms are the same (fully-connected), and just the weights differ. The initial population is of size `N` and is kept constant through all iterations.\n\n**Evaluation of fitness**. To evaluate the fitness of the organisms, NEAT clusters the population into _species_ (similar topologies are put together) so that members of the same species will have similar fitness values. Given two organisms `x` and `y`, NEAT proposes a function ` = (x, y)` that measures how similar the organisms are. Hence, with a so-called _compatibility threshold_ `_t`, it is possible to cluster the population by comparing ` < _t`. The fitness `f_x` of the organism `x`, belonging to species `s_i`, is a function of `s_i` and the reward `r_x` of the network: `f_x=f(s_i, r_x)`. The reward can be the accuracy of the network or any other measure of its performance in a given task. To see the actual functions `` and `f_x`, consult page 13 of the <a href=\"http://nn.cs.utexas.edu/keyword?stanley:ec02\" blank=\"_target\">NEAT paper</a>. \n\n**Generating the offspring**. In each generation, the `N` organisms will be replaced by the offspring. The offspring is _per species_. 25% of the offspring (`A = 0.25 * N`) results from mutation without crossover, and (presumably) 75% (`B = 0.75 * N`) from reproduction (crossover + posterior mutation). The first step for reproduction is to eliminate the lowest-performing organisms from the entire population (the \"lowest\" criteria is not mentioned in the paper, sorry). Then, for each species `s_i`, the within-species fitness sum is computed: `F_i = sum_all(f_x)` for all `x` in `s_i`. Next, we require the proportion of `F_i` with respect to all species: `o_i = F_i / sum_all(F_j)`. Each species `s_i` gets assigned a number `o_i * B` of offspring. It is mating time now!\n\nThe specifics of mutation and crossover are as follow:\n\n- **Mutation**: There are three mutation operators: _weight perturbation_, _add connection_, and _add node_. I am skipping the first one because we do not need it for WANNs. <u>Add connection</u> selects two unconnected nodes (presumably at random) and links them with a new random weight `w`. <u>Add node</u> selects an existing connection (presumably at random too) with some weight `w_c` and splits it in two, placing a new node in between. The new weights are `w_in = 1` and `w_out = w_c`. The old connection is set to `DISABLED`.\n- **Crossover**: When crossing over, genomes `x` and `y` are lined up as shown in Figure 2. If two genes `x[i]` and `y[i]` have the same `innovation number` they are called _matching genes_. If they are not matching, then they are either _disjoint_ (if the `innovation number` of `x[i]` <= `y[i]`, or vice versa) or _excess_ (if the `innovation number` of `x[i]` > `y[i]`, or vice versa). At the time of selecting the genes for the offspring, _matching genes_ are selected at random from either parent and _excess_/_disjoint_ are taken from the most-fit parent (if they are equally fit then they are chosen randomly).\n\n{% asset_img wanns-neat-crossover.png \"Figure 2. Crossover in NEAT. The top numbers represent the `innovation number`\" %}\n\n<br/>\nThere are some specifics about the NEAT implementation that you can consult on pages 14 and 15 (section: \"Performance Evaluations\") of the paper. But roughly, this is what you need to understand WANNs.\n\n### Dominance relations\n\n_Dominance_ is a concept from <a href=\"https://en.wikipedia.org/wiki/Multi-objective_optimization\" target=\"_blank\">multi-objective optimization</a>, where we want to find the optimal value (minimum or maximum) for a set of functions. That is, `min( f_1(x), f_2(x), ... , f_k(x) )`, where `k` is the number of objectives. `x* in X` is called a _feasible solution_, and `X` is the _feasible set_ of decision vectors. A vector `z* := < f_1(x*), f_2(x*), ... f_k(x*)>` for a feasible solution `x*` is called an _outcome_.\n\nA feasible solution `x1` is said to (Pareto) **dominate** a solution `x2` (for minimization) if:\n1. `f_i(x1) <= f_i(x_2)` **for all** indices `i in {1, 2, ..., k}`\n2. `f_j(x1) < f_i(x_2)` **for at least one** index `j in {1, 2, ..., k}`\n\nIn words, `x1` dominates `x2` if `x2` does not improve any of the objectives `f_i`  when compared to `x1`, and there is at least one `f_i` for which `x1` is strictly better than `x2`.\n\nA solution `x*` is called _Pareto optimal_ (a.k.a. non-dominated or Pareto efficient) if there does not exist another solution that dominates it. The set `F` of Pareto optimal solutions is called _Pareto front_.\n\nIt is possible to group the solutions `x_i` in fronts. First, using the set `X` we need to compute the first front `F_1`. Second, we compute `X = X - F1`  and compute a new front `F_2`. We can repeat this process until `X` is empty. An efficient algorithm for this process is on <a href=\"http://www.dmi.unict.it/mpavone/nc-cs/materiale/NSGA-II.pdf\" target=\"_blank\">Page 3 of this paper</a> cited by Gaier and Ha.\n\n## The approach in a nutshell\n\nGaier and Ha propose a search algorithm where the constructed **networks will not be trained**, saving us a lot of computational resources. The core of the algorithm is [NEAT](#neuroevolution-of-augmenting-topologies-neat) with some minor modifications:\n\n1. The **initial population** is **not uniform**.\n2. **For every architecture a weight** from a set `W` **is used**. **All neurons** in the architecture  **share this weight** and the network will infer with no training whatsoever. This is repeated for each weight in `W` (each trial is called a _rollout_).\n3. The **fitness considers performance** (without training) over the rollouts **and complexity** of the networks.\n4. A **modified mutation operator**.\n\nThe algorithm was tested for reinforcement learning and image classification tasks. We just need to elaborate on each of the modifications before looking at the main results.\n\n## Understanding the key elements\n\n### The initial set of architectures\n\nSimilarly to NEAT, the algorithm starts with _minimal topologies_. By looking at an example in the WANNs' paper (see Figure 3) it is possible to infer a small modification on what a minimal topology is. Instead of fully connected inputs/outputs, WANNs start from **a population of networks with zero hidden nodes, which have inputs _randomly_ connected to outputs**.\n\n{% asset_img wanns-minimal-net.png \"Figure 3: Example of a minimal topology\" %}\n\n### The rollouts\n\nThe rollouts are the key aspect of how to measure the performance of the architectures. _A rollout_ is the evaluation of a network with one shared weight. The weights considered are in the set `W = [-2, -1, -0.5, +0.5, +1, +2]`. For each weight in `W`, the network is evaluated. For RL that means obtaining a cumulative reward, and for image classification the accuracy.\n\nAfter all evaluations, three measures are computed:\n1. `f_1`: The average of the rollouts' rewards/accuracies.\n2. `f_2`: The maximum reward/accuracy among rollouts.\n3. `f_3`: The number of connections in the architecture.\n\n### The fitness values\n\nOnce the rollouts are finished, the **fitness** is measured. The authors propose a stochastic approach:\n\n- 20% of the time, the fitness is the rank after sorting by _dominance relations_ of the objectives `f_1` and `f_2`.\n- 80% of the time, the fitness is the rank after sorting by _dominance relations_ of the objectives `f_1` and `f_3`.\n\n### The mutation operators\n\nThe architectures mutate via **three operators**:\n\n- **Insert new node**: Same as in NEAT, but without weights and with an activation function sampled at random from the set `[linear, step, sin, cosine, Gaussian, tanh, sigmoid, absolute-value, negative linear, ReLU]`.\n- **Add new connection**: Same as in NEAT, but without weights.\n- **Change existing activation**: An existing node is chosen (presumably at random). Then, an activation function `F` is randomly selected from the set `[linear, step, sin, cosine, Gaussian, tanh, sigmoid, absolute-value, negative linear, ReLU]` and the node gets `F` assigned.\n\n## Experiments and results\n\nAs mentioned earlier, WANNs are tested on reinforcement learning (RL) and image classification tasks. To avoid rephrasing the paper, I limit myself to present an overview of the findings, emphasizing what I consider the most interesting. I recommend checking all the results in the <a href=\"https://weightagnostic.github.io\" target=\"_blank\">paper's interactive version</a>.\n\n### Reinforcement Learning\n\n#### Experiment setup\n\nThe reinforcement learning tasks solved with WANNs are `CartPoleSwingUp`, `BipedalWalker-v2`, and `CarRacing-v0` (all of them available as <a href=\"https://gym.openai.com/envs/\" target=\"_blank\">OpenAI Gym environments</a>). For each of the tasks, a WANN is constructed by following the approach we have discussed so far. To compare the WANN's performance, the authors use a <a href=\"https://worldmodels.github.io\" target=\"_blank\">fixed topology from literatue</a> as a baseline. For the two architectures, the next experiments are repeated 100 times per task:\n\n1. Solve the task with random weights (drawn from `Uniform(-2, 2)`) assigned per neuron.\n2. Solve the task after training (tuning) of random weights (drawn from `Uniform(-2, 2)`) assigned per neuron.\n3. Solve the task with a shared random weight (drawn from `Uniform(-2, 2)`).\n4. Solve the task after training (tuning) of a shared random weight (drawn from `Uniform(-2, 2)`).\n\nThis setting makes a lot of sense to me. By comparing the performance using non-tuned random weights, we can see how dependent an architecture is on the weights. If it performs poorly, then the architecture might not be adequate for the task and relies too much on the weights. On the other hand, when tuning the weights, important things can be revealed: 1) if the architecture performed poorly with random weights but improved with tuning, then it could suggest that the tuned weights _hide_ the fact that the architecture is not adequate by design; 2) if a WANN does not improve much when tuned, the topology itself might be \"overfitting\" (which is not necessarily a bad thing because then we could forget about _general-purpose_ architectures and start designing _ad hoc_ topologies for our tasks).\n\n#### Results\n\nThe reported rewards are shown in Figure 4. In short, WANNs outperform the baseline in 11/12 experiments. In general, the baseline appears very dependent on the tuned weights (very low or even negative rewards when randomly assigned). A WANN has the same drawback for random weights but it is way more robust to a random shared weight. \n\nFor the rest of the analysis let's forget about the baseline. A WANN achieves its maximum performance after tuning of non-shared weights, which also improves its variance. The reward obtained with random shared weights is still low compared to the maximum obtained and shows important variance. When the shared weight is tuned, the performance becomes more certain and importantly higher but not the highest.\n\n{% asset_img wanns-results-rl.png \"Figure 4: Results for the RL experiments, as reported in the original paper.\" %}\n\n<br/>\nWith these numbers, it is not clear yet what the essential advantage of a WANN is. To clear this out, Gaier and Ha help us with an analysis of the topologies discovered, from which I will simply rephrase the main claims. WANNs are simple and modular. They seem to ignore non-important inputs and use fewer connections than state-of-the-art approaches. An important note made in the paper is that the complexity of the topologies increases at each generation, capturing one by one the principal elements of the task. For example, in the `CartPoleSwingUp` task, a topology from an early generation captures the position-velocity relation, and in a later generation, the same topology adds nodes on top that refine the balancing (check the <a href=\"https://weightagnostic.github.io\" target=\"_blank\">interactive paper</a> for an animation of this behavior).\n\n### Image Classification\n\n#### Experiment setup\n\nFor Image Classification the setup changes. The task is now <a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">MNIST</a>. This time the baselines are a simple Linear Regressor and a Two-Layer CNN. Presumably, the next steps are followed 100 times too: \n\n1. Solve the task with a shared random weight (drawn from `Uniform(-2, 2)`).\n2. Solve the task after training (tuning) of a shared random weight (drawn from `Uniform(-2, 2)`).\n3. Instantiate the WNN with `m` different weights (without training) so that we end up with `m` WNNs with the same topology and just the shared weight differing. Build an ensemble where each of these networks is given a single vote so that the predicted value for an image is the digit that received the most votes.\n4. Repeat 3, but training each member of the ensemble (I assume) individually.\n\nA strong justification for trying with an ensemble in this task is not clear to me. My impression is that since the results did not show as strong as in RL, they experimented with an alternative that could strengthen WANNs. The task itself is simple to me, considering that a simple Two-Layer CNN is already a strong baseline for MNIST.\n\n#### Results\n\nThe results are in Figure 5. This time WANNs do not seem as well-performing as in the RL experiment. As I said, maybe that's the reason behind trying with an ensemble. I do not have much to say about this experiment, just that it seems to me that there is much room for improvement in the experiment setup. \n\n{% asset_img wanns-results-ic.png \"Figure 5: Results for the Image Classification experiments, as reported in the original paper.\" %}\n\n## My feelings about the paper\n\nI really like this paper. From the moment I knew about it, I was impressed for its simplicity and power. We are always worried about the best weights of a network and put too much care in their training to compare the architectures, but this work shows that a single shared weight can help us to compare and build architectures efficiently.\n\nMy main takeover is that a WANN has a more adequate topology because forgetting about the weights and training makes the algorithm to focus on the arrangement of layers. However, I am a bit worried about the variance of the random shared weight (Figure 4). Also, I am not clear on the selection of the baseline for the RL tasks. Are they convenient? Are there better baselines? I must admit I did not read the baseline's paper. If I have the time to do it, I will update this part of the summary with more insights.\n\nI like the fitness function, wich uses dominance relations. By considering different objectives, the estimation of the performance looks more solid to me. \n\nConcerning the image classification experiment, I have to say that I was expecting a bit more of discussion but it is great that Gaier and Ha spent time to show us WANNs in that domain too. To me, WANNs are not looking so powerful there, but it is also true that researchers have made much progress in image classification and sometimes is unfair for the novel approaches to compete with state-of-the-art architectures in the field, even though they have excellent ideas as it is the case of Gaier and Ha.\n\nIn general, I think it would be good to see how other search methods (e.g. RL) behave with WANNs, specially because of the recent success of NAS with RL. I would also like to see the running times, because although the search does not look expensive, solving the RL environments could be. As it is the case with some papers from the Big Guys, the results look amazing but the computational resources could be out of our reach.\n\nThis is all for my summary. I hope this helps you to understand and possibly implement the paper. Do not forget to share :)\n","slug":"wanns","published":1,"_id":"ckb41ifb9000080592xcg1xl8","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>I got aware of this paper at the NeurIPS 2019 conference in Vancouver. You can find the official paper <a href=\"https://arxiv.org/abs/1906.04358\" target=\"_blank\">here</a>, and an interactive version <a href=\"https://weightagnostic.github.io\" target=\"_blank\">at this link</a>. The source code is hosted in <a href=\"https://github.com/google/brain-tokyo-workshop/tree/master/WANNRelease\" target=\"_blank\">this GitHub repo</a>  although it is a bit hard to get through it because of the directory structure.</p>\n<p>The main highlight of the paper is that <em>artificially crafted neural networks with a single weight value  shared by all neurons and with no training  achieve competitive performance on popular baselines</em>. Such networks are labeled as Weight-Agnostic Neural Networks (WANNs). Before going through the paper, I recommend checking some basic concepts about simple neural network architectures, evolutionary algorithms, image classification, and reinforcement learning.</p>\n<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><h3 id=\"Neural-Architecture-Search-NAS\"><a href=\"#Neural-Architecture-Search-NAS\" class=\"headerlink\" title=\"Neural Architecture Search (NAS)\"></a>Neural Architecture Search (NAS)</h3><p>In a nutshell, the ultimate goal of NAS is to get rid of humans when selecting/designing the best architecture for a dataset. It does so by creating algorithms to automatically select the best arrangement of layers and the best hyperparameters. We can find a good number of papers dealing with this, and every approach has its strengths and weaknesses, but perhaps the one weakness they all share is the expensiveness of the training and evaluation of the neural networks. Why is this a problem? Because in a search process, we got to compare a significant number of neural architectures to see which ones perform best, based on its train/test accuracy. If we were to compare, lets say 10, it would not be a problem if each of the networks took 2 minutes to train, but if we compare thousands of architectures then it starts being a problem.</p>\n<blockquote>\n<p>If you want to read more about NAS, I recommend the <a href=\"https://arxiv.org/abs/1808.05377\" target=\"_blank\">survey by Elsken et al.</a></p>\n</blockquote>\n<h3 id=\"Evolutionary-Algorithms-EAs\"><a href=\"#Evolutionary-Algorithms-EAs\" class=\"headerlink\" title=\"Evolutionary Algorithms (EAs)\"></a>Evolutionary Algorithms (EAs)</h3><p>An EA is an optimization method inspired by biological evolution. In short, it claims that, for a given problem, you can start with a set of random solutions (a population), which will be altered with some operations (crossover and mutation) so that they can evolve into a better solution, similar to what nature does with species.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Evolutionary_algorithm#Implementation\" target=\"_blank\">Wikipedias pseudo-code</a> summarizes the generic EA steps as follow:</p>\n<ol>\n<li>Generate the initial population of organisms randomly (<strong>first generation</strong>).</li>\n<li><strong>Repeat</strong> the following <strong>regenerational steps</strong> until termination:<ol>\n<li><strong>Evaluate</strong> the <strong>fitness of each organism</strong> in the population (i.e., a measure of their quality).</li>\n<li><strong>Select</strong> the <strong>fittest</strong> organisms <strong>for reproduction</strong>.</li>\n<li><strong>Breed new organisms through crossover and mutation</strong> operations to give birth to offspring.</li>\n<li><strong>Evaluate</strong> the fitness of <strong>new organisms</strong>.</li>\n<li><strong>Replace</strong> the <strong>least-fit</strong> organisms of the population <strong>with new organisms</strong>.</li>\n</ol>\n</li>\n</ol>\n<p>The loop of step 2 can variate in the order of the sub-steps, but this reflects the general idea.</p>\n<h3 id=\"NeuroEvolution-of-Augmenting-Topologies-NEAT\"><a href=\"#NeuroEvolution-of-Augmenting-Topologies-NEAT\" class=\"headerlink\" title=\"NeuroEvolution of Augmenting Topologies (NEAT)\"></a>NeuroEvolution of Augmenting Topologies (NEAT)</h3><p><a href=\"http://nn.cs.utexas.edu/keyword?stanley:ec02\" target=\"_blank\" rel=\"noopener\" blank=\"_target\">NEAT (2002)</a> is an EA approach to NAS. The algorithm for Weight-Agnostic Neural Networks is highly inspired by it, thus it is convenient to take a quick look at it.</p>\n<p>NEAT encodes the networks (i.e., the organisms) into <em>genomes</em>, as shown in Figure 1. A <em>genome</em> contains a list of <em>connection genes</em> which capture the topology of the network (i.e., the architecture). Each element in the list has 4 main attributes: <code>in</code> node, <code>out</code> node, <code>weight</code>, and <code>innovation number</code> (a counter of the latest generation in which the node was mutated). The remaining attribute (<code>enabled</code>) will help during the mutations because connections can change.</p>\n<img src=\"/2020/03/09/wanns/wanns-neat-genome.png\" class=\"\" title=\"Figure 1: Example of a Genome in NEAT\">\n\n<br/>\nNEAT's goal is to give birth to the best possible network with the next pseudo-code:\n\n<ol>\n<li>Generate <strong>initial population</strong>.</li>\n<li><strong>Repeat</strong> <code>m</code> times:<ol>\n<li><strong>Compute the fitness</strong> of the organisms.</li>\n<li>Generate <strong>offspring</strong> according to fitness.</li>\n<li><strong>Replace the population</strong> with the offspring.</li>\n</ol>\n</li>\n</ol>\n<p>Lets understand each of the key elements.</p>\n<p><strong>The initial population</strong>. NEAT considers a <em>uniform</em> first generation of <em>minimal networks</em>. A <em>minimal network</em> is one with only inputs and outputs (i.e., no hidden nodes). <em>Uniform</em> seems to mean that all the organisms are the same (fully-connected), and just the weights differ. The initial population is of size <code>N</code> and is kept constant through all iterations.</p>\n<p><strong>Evaluation of fitness</strong>. To evaluate the fitness of the organisms, NEAT clusters the population into <em>species</em> (similar topologies are put together) so that members of the same species will have similar fitness values. Given two organisms <code>x</code> and <code>y</code>, NEAT proposes a function <code> = (x, y)</code> that measures how similar the organisms are. Hence, with a so-called <em>compatibility threshold</em> <code>_t</code>, it is possible to cluster the population by comparing <code> &lt; _t</code>. The fitness <code>f_x</code> of the organism <code>x</code>, belonging to species <code>s_i</code>, is a function of <code>s_i</code> and the reward <code>r_x</code> of the network: <code>f_x=f(s_i, r_x)</code>. The reward can be the accuracy of the network or any other measure of its performance in a given task. To see the actual functions <code></code> and <code>f_x</code>, consult page 13 of the <a href=\"http://nn.cs.utexas.edu/keyword?stanley:ec02\" target=\"_blank\" rel=\"noopener\" blank=\"_target\">NEAT paper</a>. </p>\n<p><strong>Generating the offspring</strong>. In each generation, the <code>N</code> organisms will be replaced by the offspring. The offspring is <em>per species</em>. 25% of the offspring (<code>A = 0.25 * N</code>) results from mutation without crossover, and (presumably) 75% (<code>B = 0.75 * N</code>) from reproduction (crossover + posterior mutation). The first step for reproduction is to eliminate the lowest-performing organisms from the entire population (the lowest criteria is not mentioned in the paper, sorry). Then, for each species <code>s_i</code>, the within-species fitness sum is computed: <code>F_i = sum_all(f_x)</code> for all <code>x</code> in <code>s_i</code>. Next, we require the proportion of <code>F_i</code> with respect to all species: <code>o_i = F_i / sum_all(F_j)</code>. Each species <code>s_i</code> gets assigned a number <code>o_i * B</code> of offspring. It is mating time now!</p>\n<p>The specifics of mutation and crossover are as follow:</p>\n<ul>\n<li><strong>Mutation</strong>: There are three mutation operators: <em>weight perturbation</em>, <em>add connection</em>, and <em>add node</em>. I am skipping the first one because we do not need it for WANNs. <u>Add connection</u> selects two unconnected nodes (presumably at random) and links them with a new random weight <code>w</code>. <u>Add node</u> selects an existing connection (presumably at random too) with some weight <code>w_c</code> and splits it in two, placing a new node in between. The new weights are <code>w_in = 1</code> and <code>w_out = w_c</code>. The old connection is set to <code>DISABLED</code>.</li>\n<li><strong>Crossover</strong>: When crossing over, genomes <code>x</code> and <code>y</code> are lined up as shown in Figure 2. If two genes <code>x[i]</code> and <code>y[i]</code> have the same <code>innovation number</code> they are called <em>matching genes</em>. If they are not matching, then they are either <em>disjoint</em> (if the <code>innovation number</code> of <code>x[i]</code> &lt;= <code>y[i]</code>, or vice versa) or <em>excess</em> (if the <code>innovation number</code> of <code>x[i]</code> &gt; <code>y[i]</code>, or vice versa). At the time of selecting the genes for the offspring, <em>matching genes</em> are selected at random from either parent and <em>excess</em>/<em>disjoint</em> are taken from the most-fit parent (if they are equally fit then they are chosen randomly).</li>\n</ul>\n<img src=\"/2020/03/09/wanns/wanns-neat-crossover.png\" class=\"\" title=\"Figure 2. Crossover in NEAT. The top numbers represent the &#96;innovation number&#96;\">\n\n<br/>\nThere are some specifics about the NEAT implementation that you can consult on pages 14 and 15 (section: \"Performance Evaluations\") of the paper. But roughly, this is what you need to understand WANNs.\n\n<h3 id=\"Dominance-relations\"><a href=\"#Dominance-relations\" class=\"headerlink\" title=\"Dominance relations\"></a>Dominance relations</h3><p><em>Dominance</em> is a concept from <a href=\"https://en.wikipedia.org/wiki/Multi-objective_optimization\" target=\"_blank\">multi-objective optimization</a>, where we want to find the optimal value (minimum or maximum) for a set of functions. That is, <code>min( f_1(x), f_2(x), ... , f_k(x) )</code>, where <code>k</code> is the number of objectives. <code>x* in X</code> is called a <em>feasible solution</em>, and <code>X</code> is the <em>feasible set</em> of decision vectors. A vector <code>z* := &lt; f_1(x*), f_2(x*), ... f_k(x*)&gt;</code> for a feasible solution <code>x*</code> is called an <em>outcome</em>.</p>\n<p>A feasible solution <code>x1</code> is said to (Pareto) <strong>dominate</strong> a solution <code>x2</code> (for minimization) if:</p>\n<ol>\n<li><code>f_i(x1) &lt;= f_i(x_2)</code> <strong>for all</strong> indices <code>i in {1, 2, ..., k}</code></li>\n<li><code>f_j(x1) &lt; f_i(x_2)</code> <strong>for at least one</strong> index <code>j in {1, 2, ..., k}</code></li>\n</ol>\n<p>In words, <code>x1</code> dominates <code>x2</code> if <code>x2</code> does not improve any of the objectives <code>f_i</code>  when compared to <code>x1</code>, and there is at least one <code>f_i</code> for which <code>x1</code> is strictly better than <code>x2</code>.</p>\n<p>A solution <code>x*</code> is called <em>Pareto optimal</em> (a.k.a. non-dominated or Pareto efficient) if there does not exist another solution that dominates it. The set <code>F</code> of Pareto optimal solutions is called <em>Pareto front</em>.</p>\n<p>It is possible to group the solutions <code>x_i</code> in fronts. First, using the set <code>X</code> we need to compute the first front <code>F_1</code>. Second, we compute <code>X = X - F1</code>  and compute a new front <code>F_2</code>. We can repeat this process until <code>X</code> is empty. An efficient algorithm for this process is on <a href=\"http://www.dmi.unict.it/mpavone/nc-cs/materiale/NSGA-II.pdf\" target=\"_blank\">Page 3 of this paper</a> cited by Gaier and Ha.</p>\n<h2 id=\"The-approach-in-a-nutshell\"><a href=\"#The-approach-in-a-nutshell\" class=\"headerlink\" title=\"The approach in a nutshell\"></a>The approach in a nutshell</h2><p>Gaier and Ha propose a search algorithm where the constructed <strong>networks will not be trained</strong>, saving us a lot of computational resources. The core of the algorithm is <a href=\"#neuroevolution-of-augmenting-topologies-neat\">NEAT</a> with some minor modifications:</p>\n<ol>\n<li>The <strong>initial population</strong> is <strong>not uniform</strong>.</li>\n<li><strong>For every architecture a weight</strong> from a set <code>W</code> <strong>is used</strong>. <strong>All neurons</strong> in the architecture  <strong>share this weight</strong> and the network will infer with no training whatsoever. This is repeated for each weight in <code>W</code> (each trial is called a <em>rollout</em>).</li>\n<li>The <strong>fitness considers performance</strong> (without training) over the rollouts <strong>and complexity</strong> of the networks.</li>\n<li>A <strong>modified mutation operator</strong>.</li>\n</ol>\n<p>The algorithm was tested for reinforcement learning and image classification tasks. We just need to elaborate on each of the modifications before looking at the main results.</p>\n<h2 id=\"Understanding-the-key-elements\"><a href=\"#Understanding-the-key-elements\" class=\"headerlink\" title=\"Understanding the key elements\"></a>Understanding the key elements</h2><h3 id=\"The-initial-set-of-architectures\"><a href=\"#The-initial-set-of-architectures\" class=\"headerlink\" title=\"The initial set of architectures\"></a>The initial set of architectures</h3><p>Similarly to NEAT, the algorithm starts with <em>minimal topologies</em>. By looking at an example in the WANNs paper (see Figure 3) it is possible to infer a small modification on what a minimal topology is. Instead of fully connected inputs/outputs, WANNs start from <strong>a population of networks with zero hidden nodes, which have inputs <em>randomly</em> connected to outputs</strong>.</p>\n<img src=\"/2020/03/09/wanns/wanns-minimal-net.png\" class=\"\" title=\"Figure 3: Example of a minimal topology\">\n\n<h3 id=\"The-rollouts\"><a href=\"#The-rollouts\" class=\"headerlink\" title=\"The rollouts\"></a>The rollouts</h3><p>The rollouts are the key aspect of how to measure the performance of the architectures. <em>A rollout</em> is the evaluation of a network with one shared weight. The weights considered are in the set <code>W = [-2, -1, -0.5, +0.5, +1, +2]</code>. For each weight in <code>W</code>, the network is evaluated. For RL that means obtaining a cumulative reward, and for image classification the accuracy.</p>\n<p>After all evaluations, three measures are computed:</p>\n<ol>\n<li><code>f_1</code>: The average of the rollouts rewards/accuracies.</li>\n<li><code>f_2</code>: The maximum reward/accuracy among rollouts.</li>\n<li><code>f_3</code>: The number of connections in the architecture.</li>\n</ol>\n<h3 id=\"The-fitness-values\"><a href=\"#The-fitness-values\" class=\"headerlink\" title=\"The fitness values\"></a>The fitness values</h3><p>Once the rollouts are finished, the <strong>fitness</strong> is measured. The authors propose a stochastic approach:</p>\n<ul>\n<li>20% of the time, the fitness is the rank after sorting by <em>dominance relations</em> of the objectives <code>f_1</code> and <code>f_2</code>.</li>\n<li>80% of the time, the fitness is the rank after sorting by <em>dominance relations</em> of the objectives <code>f_1</code> and <code>f_3</code>.</li>\n</ul>\n<h3 id=\"The-mutation-operators\"><a href=\"#The-mutation-operators\" class=\"headerlink\" title=\"The mutation operators\"></a>The mutation operators</h3><p>The architectures mutate via <strong>three operators</strong>:</p>\n<ul>\n<li><strong>Insert new node</strong>: Same as in NEAT, but without weights and with an activation function sampled at random from the set <code>[linear, step, sin, cosine, Gaussian, tanh, sigmoid, absolute-value, negative linear, ReLU]</code>.</li>\n<li><strong>Add new connection</strong>: Same as in NEAT, but without weights.</li>\n<li><strong>Change existing activation</strong>: An existing node is chosen (presumably at random). Then, an activation function <code>F</code> is randomly selected from the set <code>[linear, step, sin, cosine, Gaussian, tanh, sigmoid, absolute-value, negative linear, ReLU]</code> and the node gets <code>F</code> assigned.</li>\n</ul>\n<h2 id=\"Experiments-and-results\"><a href=\"#Experiments-and-results\" class=\"headerlink\" title=\"Experiments and results\"></a>Experiments and results</h2><p>As mentioned earlier, WANNs are tested on reinforcement learning (RL) and image classification tasks. To avoid rephrasing the paper, I limit myself to present an overview of the findings, emphasizing what I consider the most interesting. I recommend checking all the results in the <a href=\"https://weightagnostic.github.io\" target=\"_blank\">papers interactive version</a>.</p>\n<h3 id=\"Reinforcement-Learning\"><a href=\"#Reinforcement-Learning\" class=\"headerlink\" title=\"Reinforcement Learning\"></a>Reinforcement Learning</h3><h4 id=\"Experiment-setup\"><a href=\"#Experiment-setup\" class=\"headerlink\" title=\"Experiment setup\"></a>Experiment setup</h4><p>The reinforcement learning tasks solved with WANNs are <code>CartPoleSwingUp</code>, <code>BipedalWalker-v2</code>, and <code>CarRacing-v0</code> (all of them available as <a href=\"https://gym.openai.com/envs/\" target=\"_blank\">OpenAI Gym environments</a>). For each of the tasks, a WANN is constructed by following the approach we have discussed so far. To compare the WANNs performance, the authors use a <a href=\"https://worldmodels.github.io\" target=\"_blank\">fixed topology from literatue</a> as a baseline. For the two architectures, the next experiments are repeated 100 times per task:</p>\n<ol>\n<li>Solve the task with random weights (drawn from <code>Uniform(-2, 2)</code>) assigned per neuron.</li>\n<li>Solve the task after training (tuning) of random weights (drawn from <code>Uniform(-2, 2)</code>) assigned per neuron.</li>\n<li>Solve the task with a shared random weight (drawn from <code>Uniform(-2, 2)</code>).</li>\n<li>Solve the task after training (tuning) of a shared random weight (drawn from <code>Uniform(-2, 2)</code>).</li>\n</ol>\n<p>This setting makes a lot of sense to me. By comparing the performance using non-tuned random weights, we can see how dependent an architecture is on the weights. If it performs poorly, then the architecture might not be adequate for the task and relies too much on the weights. On the other hand, when tuning the weights, important things can be revealed: 1) if the architecture performed poorly with random weights but improved with tuning, then it could suggest that the tuned weights <em>hide</em> the fact that the architecture is not adequate by design; 2) if a WANN does not improve much when tuned, the topology itself might be overfitting (which is not necessarily a bad thing because then we could forget about <em>general-purpose</em> architectures and start designing <em>ad hoc</em> topologies for our tasks).</p>\n<h4 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h4><p>The reported rewards are shown in Figure 4. In short, WANNs outperform the baseline in 11/12 experiments. In general, the baseline appears very dependent on the tuned weights (very low or even negative rewards when randomly assigned). A WANN has the same drawback for random weights but it is way more robust to a random shared weight. </p>\n<p>For the rest of the analysis lets forget about the baseline. A WANN achieves its maximum performance after tuning of non-shared weights, which also improves its variance. The reward obtained with random shared weights is still low compared to the maximum obtained and shows important variance. When the shared weight is tuned, the performance becomes more certain and importantly higher but not the highest.</p>\n<img src=\"/2020/03/09/wanns/wanns-results-rl.png\" class=\"\" title=\"Figure 4: Results for the RL experiments, as reported in the original paper.\">\n\n<br/>\nWith these numbers, it is not clear yet what the essential advantage of a WANN is. To clear this out, Gaier and Ha help us with an analysis of the topologies discovered, from which I will simply rephrase the main claims. WANNs are simple and modular. They seem to ignore non-important inputs and use fewer connections than state-of-the-art approaches. An important note made in the paper is that the complexity of the topologies increases at each generation, capturing one by one the principal elements of the task. For example, in the `CartPoleSwingUp` task, a topology from an early generation captures the position-velocity relation, and in a later generation, the same topology adds nodes on top that refine the balancing (check the <a href=\"https://weightagnostic.github.io\" target=\"_blank\">interactive paper</a> for an animation of this behavior).\n\n<h3 id=\"Image-Classification\"><a href=\"#Image-Classification\" class=\"headerlink\" title=\"Image Classification\"></a>Image Classification</h3><h4 id=\"Experiment-setup-1\"><a href=\"#Experiment-setup-1\" class=\"headerlink\" title=\"Experiment setup\"></a>Experiment setup</h4><p>For Image Classification the setup changes. The task is now <a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">MNIST</a>. This time the baselines are a simple Linear Regressor and a Two-Layer CNN. Presumably, the next steps are followed 100 times too: </p>\n<ol>\n<li>Solve the task with a shared random weight (drawn from <code>Uniform(-2, 2)</code>).</li>\n<li>Solve the task after training (tuning) of a shared random weight (drawn from <code>Uniform(-2, 2)</code>).</li>\n<li>Instantiate the WNN with <code>m</code> different weights (without training) so that we end up with <code>m</code> WNNs with the same topology and just the shared weight differing. Build an ensemble where each of these networks is given a single vote so that the predicted value for an image is the digit that received the most votes.</li>\n<li>Repeat 3, but training each member of the ensemble (I assume) individually.</li>\n</ol>\n<p>A strong justification for trying with an ensemble in this task is not clear to me. My impression is that since the results did not show as strong as in RL, they experimented with an alternative that could strengthen WANNs. The task itself is simple to me, considering that a simple Two-Layer CNN is already a strong baseline for MNIST.</p>\n<h4 id=\"Results-1\"><a href=\"#Results-1\" class=\"headerlink\" title=\"Results\"></a>Results</h4><p>The results are in Figure 5. This time WANNs do not seem as well-performing as in the RL experiment. As I said, maybe thats the reason behind trying with an ensemble. I do not have much to say about this experiment, just that it seems to me that there is much room for improvement in the experiment setup. </p>\n<img src=\"/2020/03/09/wanns/wanns-results-ic.png\" class=\"\" title=\"Figure 5: Results for the Image Classification experiments, as reported in the original paper.\">\n\n<h2 id=\"My-feelings-about-the-paper\"><a href=\"#My-feelings-about-the-paper\" class=\"headerlink\" title=\"My feelings about the paper\"></a>My feelings about the paper</h2><p>I really like this paper. From the moment I knew about it, I was impressed for its simplicity and power. We are always worried about the best weights of a network and put too much care in their training to compare the architectures, but this work shows that a single shared weight can help us to compare and build architectures efficiently.</p>\n<p>My main takeover is that a WANN has a more adequate topology because forgetting about the weights and training makes the algorithm to focus on the arrangement of layers. However, I am a bit worried about the variance of the random shared weight (Figure 4). Also, I am not clear on the selection of the baseline for the RL tasks. Are they convenient? Are there better baselines? I must admit I did not read the baselines paper. If I have the time to do it, I will update this part of the summary with more insights.</p>\n<p>I like the fitness function, wich uses dominance relations. By considering different objectives, the estimation of the performance looks more solid to me. </p>\n<p>Concerning the image classification experiment, I have to say that I was expecting a bit more of discussion but it is great that Gaier and Ha spent time to show us WANNs in that domain too. To me, WANNs are not looking so powerful there, but it is also true that researchers have made much progress in image classification and sometimes is unfair for the novel approaches to compete with state-of-the-art architectures in the field, even though they have excellent ideas as it is the case of Gaier and Ha.</p>\n<p>In general, I think it would be good to see how other search methods (e.g. RL) behave with WANNs, specially because of the recent success of NAS with RL. I would also like to see the running times, because although the search does not look expensive, solving the RL environments could be. As it is the case with some papers from the Big Guys, the results look amazing but the computational resources could be out of our reach.</p>\n<p>This is all for my summary. I hope this helps you to understand and possibly implement the paper. Do not forget to share :)</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>I got aware of this paper at the NeurIPS 2019 conference in Vancouver. You can find the official paper <a href=\"https://arxiv.org/abs/1906.04358\" target=\"_blank\">here</a>, and an interactive version <a href=\"https://weightagnostic.github.io\" target=\"_blank\">at this link</a>. The source code is hosted in <a href=\"https://github.com/google/brain-tokyo-workshop/tree/master/WANNRelease\" target=\"_blank\">this GitHub repo</a>  although it is a bit hard to get through it because of the directory structure.</p>\n<p>The main highlight of the paper is that <em>artificially crafted neural networks with a single weight value  shared by all neurons and with no training  achieve competitive performance on popular baselines</em>. Such networks are labeled as Weight-Agnostic Neural Networks (WANNs). Before going through the paper, I recommend checking some basic concepts about simple neural network architectures, evolutionary algorithms, image classification, and reinforcement learning.</p>\n<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><h3 id=\"Neural-Architecture-Search-NAS\"><a href=\"#Neural-Architecture-Search-NAS\" class=\"headerlink\" title=\"Neural Architecture Search (NAS)\"></a>Neural Architecture Search (NAS)</h3><p>In a nutshell, the ultimate goal of NAS is to get rid of humans when selecting/designing the best architecture for a dataset. It does so by creating algorithms to automatically select the best arrangement of layers and the best hyperparameters. We can find a good number of papers dealing with this, and every approach has its strengths and weaknesses, but perhaps the one weakness they all share is the expensiveness of the training and evaluation of the neural networks. Why is this a problem? Because in a search process, we got to compare a significant number of neural architectures to see which ones perform best, based on its train/test accuracy. If we were to compare, lets say 10, it would not be a problem if each of the networks took 2 minutes to train, but if we compare thousands of architectures then it starts being a problem.</p>\n<blockquote>\n<p>If you want to read more about NAS, I recommend the <a href=\"https://arxiv.org/abs/1808.05377\" target=\"_blank\">survey by Elsken et al.</a></p>\n</blockquote>\n<h3 id=\"Evolutionary-Algorithms-EAs\"><a href=\"#Evolutionary-Algorithms-EAs\" class=\"headerlink\" title=\"Evolutionary Algorithms (EAs)\"></a>Evolutionary Algorithms (EAs)</h3><p>An EA is an optimization method inspired by biological evolution. In short, it claims that, for a given problem, you can start with a set of random solutions (a population), which will be altered with some operations (crossover and mutation) so that they can evolve into a better solution, similar to what nature does with species.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Evolutionary_algorithm#Implementation\" target=\"_blank\">Wikipedias pseudo-code</a> summarizes the generic EA steps as follow:</p>\n<ol>\n<li>Generate the initial population of organisms randomly (<strong>first generation</strong>).</li>\n<li><strong>Repeat</strong> the following <strong>regenerational steps</strong> until termination:<ol>\n<li><strong>Evaluate</strong> the <strong>fitness of each organism</strong> in the population (i.e., a measure of their quality).</li>\n<li><strong>Select</strong> the <strong>fittest</strong> organisms <strong>for reproduction</strong>.</li>\n<li><strong>Breed new organisms through crossover and mutation</strong> operations to give birth to offspring.</li>\n<li><strong>Evaluate</strong> the fitness of <strong>new organisms</strong>.</li>\n<li><strong>Replace</strong> the <strong>least-fit</strong> organisms of the population <strong>with new organisms</strong>.</li>\n</ol>\n</li>\n</ol>\n<p>The loop of step 2 can variate in the order of the sub-steps, but this reflects the general idea.</p>\n<h3 id=\"NeuroEvolution-of-Augmenting-Topologies-NEAT\"><a href=\"#NeuroEvolution-of-Augmenting-Topologies-NEAT\" class=\"headerlink\" title=\"NeuroEvolution of Augmenting Topologies (NEAT)\"></a>NeuroEvolution of Augmenting Topologies (NEAT)</h3><p><a href=\"http://nn.cs.utexas.edu/keyword?stanley:ec02\" target=\"_blank\" rel=\"noopener\" blank=\"_target\">NEAT (2002)</a> is an EA approach to NAS. The algorithm for Weight-Agnostic Neural Networks is highly inspired by it, thus it is convenient to take a quick look at it.</p>\n<p>NEAT encodes the networks (i.e., the organisms) into <em>genomes</em>, as shown in Figure 1. A <em>genome</em> contains a list of <em>connection genes</em> which capture the topology of the network (i.e., the architecture). Each element in the list has 4 main attributes: <code>in</code> node, <code>out</code> node, <code>weight</code>, and <code>innovation number</code> (a counter of the latest generation in which the node was mutated). The remaining attribute (<code>enabled</code>) will help during the mutations because connections can change.</p>\n<img src=\"/2020/03/09/wanns/wanns-neat-genome.png\" class=\"\" title=\"Figure 1: Example of a Genome in NEAT\">\n\n<br/>\nNEAT's goal is to give birth to the best possible network with the next pseudo-code:\n\n<ol>\n<li>Generate <strong>initial population</strong>.</li>\n<li><strong>Repeat</strong> <code>m</code> times:<ol>\n<li><strong>Compute the fitness</strong> of the organisms.</li>\n<li>Generate <strong>offspring</strong> according to fitness.</li>\n<li><strong>Replace the population</strong> with the offspring.</li>\n</ol>\n</li>\n</ol>\n<p>Lets understand each of the key elements.</p>\n<p><strong>The initial population</strong>. NEAT considers a <em>uniform</em> first generation of <em>minimal networks</em>. A <em>minimal network</em> is one with only inputs and outputs (i.e., no hidden nodes). <em>Uniform</em> seems to mean that all the organisms are the same (fully-connected), and just the weights differ. The initial population is of size <code>N</code> and is kept constant through all iterations.</p>\n<p><strong>Evaluation of fitness</strong>. To evaluate the fitness of the organisms, NEAT clusters the population into <em>species</em> (similar topologies are put together) so that members of the same species will have similar fitness values. Given two organisms <code>x</code> and <code>y</code>, NEAT proposes a function <code> = (x, y)</code> that measures how similar the organisms are. Hence, with a so-called <em>compatibility threshold</em> <code>_t</code>, it is possible to cluster the population by comparing <code> &lt; _t</code>. The fitness <code>f_x</code> of the organism <code>x</code>, belonging to species <code>s_i</code>, is a function of <code>s_i</code> and the reward <code>r_x</code> of the network: <code>f_x=f(s_i, r_x)</code>. The reward can be the accuracy of the network or any other measure of its performance in a given task. To see the actual functions <code></code> and <code>f_x</code>, consult page 13 of the <a href=\"http://nn.cs.utexas.edu/keyword?stanley:ec02\" target=\"_blank\" rel=\"noopener\" blank=\"_target\">NEAT paper</a>. </p>\n<p><strong>Generating the offspring</strong>. In each generation, the <code>N</code> organisms will be replaced by the offspring. The offspring is <em>per species</em>. 25% of the offspring (<code>A = 0.25 * N</code>) results from mutation without crossover, and (presumably) 75% (<code>B = 0.75 * N</code>) from reproduction (crossover + posterior mutation). The first step for reproduction is to eliminate the lowest-performing organisms from the entire population (the lowest criteria is not mentioned in the paper, sorry). Then, for each species <code>s_i</code>, the within-species fitness sum is computed: <code>F_i = sum_all(f_x)</code> for all <code>x</code> in <code>s_i</code>. Next, we require the proportion of <code>F_i</code> with respect to all species: <code>o_i = F_i / sum_all(F_j)</code>. Each species <code>s_i</code> gets assigned a number <code>o_i * B</code> of offspring. It is mating time now!</p>\n<p>The specifics of mutation and crossover are as follow:</p>\n<ul>\n<li><strong>Mutation</strong>: There are three mutation operators: <em>weight perturbation</em>, <em>add connection</em>, and <em>add node</em>. I am skipping the first one because we do not need it for WANNs. <u>Add connection</u> selects two unconnected nodes (presumably at random) and links them with a new random weight <code>w</code>. <u>Add node</u> selects an existing connection (presumably at random too) with some weight <code>w_c</code> and splits it in two, placing a new node in between. The new weights are <code>w_in = 1</code> and <code>w_out = w_c</code>. The old connection is set to <code>DISABLED</code>.</li>\n<li><strong>Crossover</strong>: When crossing over, genomes <code>x</code> and <code>y</code> are lined up as shown in Figure 2. If two genes <code>x[i]</code> and <code>y[i]</code> have the same <code>innovation number</code> they are called <em>matching genes</em>. If they are not matching, then they are either <em>disjoint</em> (if the <code>innovation number</code> of <code>x[i]</code> &lt;= <code>y[i]</code>, or vice versa) or <em>excess</em> (if the <code>innovation number</code> of <code>x[i]</code> &gt; <code>y[i]</code>, or vice versa). At the time of selecting the genes for the offspring, <em>matching genes</em> are selected at random from either parent and <em>excess</em>/<em>disjoint</em> are taken from the most-fit parent (if they are equally fit then they are chosen randomly).</li>\n</ul>\n<img src=\"/2020/03/09/wanns/wanns-neat-crossover.png\" class=\"\" title=\"Figure 2. Crossover in NEAT. The top numbers represent the &#96;innovation number&#96;\">\n\n<br/>\nThere are some specifics about the NEAT implementation that you can consult on pages 14 and 15 (section: \"Performance Evaluations\") of the paper. But roughly, this is what you need to understand WANNs.\n\n<h3 id=\"Dominance-relations\"><a href=\"#Dominance-relations\" class=\"headerlink\" title=\"Dominance relations\"></a>Dominance relations</h3><p><em>Dominance</em> is a concept from <a href=\"https://en.wikipedia.org/wiki/Multi-objective_optimization\" target=\"_blank\">multi-objective optimization</a>, where we want to find the optimal value (minimum or maximum) for a set of functions. That is, <code>min( f_1(x), f_2(x), ... , f_k(x) )</code>, where <code>k</code> is the number of objectives. <code>x* in X</code> is called a <em>feasible solution</em>, and <code>X</code> is the <em>feasible set</em> of decision vectors. A vector <code>z* := &lt; f_1(x*), f_2(x*), ... f_k(x*)&gt;</code> for a feasible solution <code>x*</code> is called an <em>outcome</em>.</p>\n<p>A feasible solution <code>x1</code> is said to (Pareto) <strong>dominate</strong> a solution <code>x2</code> (for minimization) if:</p>\n<ol>\n<li><code>f_i(x1) &lt;= f_i(x_2)</code> <strong>for all</strong> indices <code>i in {1, 2, ..., k}</code></li>\n<li><code>f_j(x1) &lt; f_i(x_2)</code> <strong>for at least one</strong> index <code>j in {1, 2, ..., k}</code></li>\n</ol>\n<p>In words, <code>x1</code> dominates <code>x2</code> if <code>x2</code> does not improve any of the objectives <code>f_i</code>  when compared to <code>x1</code>, and there is at least one <code>f_i</code> for which <code>x1</code> is strictly better than <code>x2</code>.</p>\n<p>A solution <code>x*</code> is called <em>Pareto optimal</em> (a.k.a. non-dominated or Pareto efficient) if there does not exist another solution that dominates it. The set <code>F</code> of Pareto optimal solutions is called <em>Pareto front</em>.</p>\n<p>It is possible to group the solutions <code>x_i</code> in fronts. First, using the set <code>X</code> we need to compute the first front <code>F_1</code>. Second, we compute <code>X = X - F1</code>  and compute a new front <code>F_2</code>. We can repeat this process until <code>X</code> is empty. An efficient algorithm for this process is on <a href=\"http://www.dmi.unict.it/mpavone/nc-cs/materiale/NSGA-II.pdf\" target=\"_blank\">Page 3 of this paper</a> cited by Gaier and Ha.</p>\n<h2 id=\"The-approach-in-a-nutshell\"><a href=\"#The-approach-in-a-nutshell\" class=\"headerlink\" title=\"The approach in a nutshell\"></a>The approach in a nutshell</h2><p>Gaier and Ha propose a search algorithm where the constructed <strong>networks will not be trained</strong>, saving us a lot of computational resources. The core of the algorithm is <a href=\"#neuroevolution-of-augmenting-topologies-neat\">NEAT</a> with some minor modifications:</p>\n<ol>\n<li>The <strong>initial population</strong> is <strong>not uniform</strong>.</li>\n<li><strong>For every architecture a weight</strong> from a set <code>W</code> <strong>is used</strong>. <strong>All neurons</strong> in the architecture  <strong>share this weight</strong> and the network will infer with no training whatsoever. This is repeated for each weight in <code>W</code> (each trial is called a <em>rollout</em>).</li>\n<li>The <strong>fitness considers performance</strong> (without training) over the rollouts <strong>and complexity</strong> of the networks.</li>\n<li>A <strong>modified mutation operator</strong>.</li>\n</ol>\n<p>The algorithm was tested for reinforcement learning and image classification tasks. We just need to elaborate on each of the modifications before looking at the main results.</p>\n<h2 id=\"Understanding-the-key-elements\"><a href=\"#Understanding-the-key-elements\" class=\"headerlink\" title=\"Understanding the key elements\"></a>Understanding the key elements</h2><h3 id=\"The-initial-set-of-architectures\"><a href=\"#The-initial-set-of-architectures\" class=\"headerlink\" title=\"The initial set of architectures\"></a>The initial set of architectures</h3><p>Similarly to NEAT, the algorithm starts with <em>minimal topologies</em>. By looking at an example in the WANNs paper (see Figure 3) it is possible to infer a small modification on what a minimal topology is. Instead of fully connected inputs/outputs, WANNs start from <strong>a population of networks with zero hidden nodes, which have inputs <em>randomly</em> connected to outputs</strong>.</p>\n<img src=\"/2020/03/09/wanns/wanns-minimal-net.png\" class=\"\" title=\"Figure 3: Example of a minimal topology\">\n\n<h3 id=\"The-rollouts\"><a href=\"#The-rollouts\" class=\"headerlink\" title=\"The rollouts\"></a>The rollouts</h3><p>The rollouts are the key aspect of how to measure the performance of the architectures. <em>A rollout</em> is the evaluation of a network with one shared weight. The weights considered are in the set <code>W = [-2, -1, -0.5, +0.5, +1, +2]</code>. For each weight in <code>W</code>, the network is evaluated. For RL that means obtaining a cumulative reward, and for image classification the accuracy.</p>\n<p>After all evaluations, three measures are computed:</p>\n<ol>\n<li><code>f_1</code>: The average of the rollouts rewards/accuracies.</li>\n<li><code>f_2</code>: The maximum reward/accuracy among rollouts.</li>\n<li><code>f_3</code>: The number of connections in the architecture.</li>\n</ol>\n<h3 id=\"The-fitness-values\"><a href=\"#The-fitness-values\" class=\"headerlink\" title=\"The fitness values\"></a>The fitness values</h3><p>Once the rollouts are finished, the <strong>fitness</strong> is measured. The authors propose a stochastic approach:</p>\n<ul>\n<li>20% of the time, the fitness is the rank after sorting by <em>dominance relations</em> of the objectives <code>f_1</code> and <code>f_2</code>.</li>\n<li>80% of the time, the fitness is the rank after sorting by <em>dominance relations</em> of the objectives <code>f_1</code> and <code>f_3</code>.</li>\n</ul>\n<h3 id=\"The-mutation-operators\"><a href=\"#The-mutation-operators\" class=\"headerlink\" title=\"The mutation operators\"></a>The mutation operators</h3><p>The architectures mutate via <strong>three operators</strong>:</p>\n<ul>\n<li><strong>Insert new node</strong>: Same as in NEAT, but without weights and with an activation function sampled at random from the set <code>[linear, step, sin, cosine, Gaussian, tanh, sigmoid, absolute-value, negative linear, ReLU]</code>.</li>\n<li><strong>Add new connection</strong>: Same as in NEAT, but without weights.</li>\n<li><strong>Change existing activation</strong>: An existing node is chosen (presumably at random). Then, an activation function <code>F</code> is randomly selected from the set <code>[linear, step, sin, cosine, Gaussian, tanh, sigmoid, absolute-value, negative linear, ReLU]</code> and the node gets <code>F</code> assigned.</li>\n</ul>\n<h2 id=\"Experiments-and-results\"><a href=\"#Experiments-and-results\" class=\"headerlink\" title=\"Experiments and results\"></a>Experiments and results</h2><p>As mentioned earlier, WANNs are tested on reinforcement learning (RL) and image classification tasks. To avoid rephrasing the paper, I limit myself to present an overview of the findings, emphasizing what I consider the most interesting. I recommend checking all the results in the <a href=\"https://weightagnostic.github.io\" target=\"_blank\">papers interactive version</a>.</p>\n<h3 id=\"Reinforcement-Learning\"><a href=\"#Reinforcement-Learning\" class=\"headerlink\" title=\"Reinforcement Learning\"></a>Reinforcement Learning</h3><h4 id=\"Experiment-setup\"><a href=\"#Experiment-setup\" class=\"headerlink\" title=\"Experiment setup\"></a>Experiment setup</h4><p>The reinforcement learning tasks solved with WANNs are <code>CartPoleSwingUp</code>, <code>BipedalWalker-v2</code>, and <code>CarRacing-v0</code> (all of them available as <a href=\"https://gym.openai.com/envs/\" target=\"_blank\">OpenAI Gym environments</a>). For each of the tasks, a WANN is constructed by following the approach we have discussed so far. To compare the WANNs performance, the authors use a <a href=\"https://worldmodels.github.io\" target=\"_blank\">fixed topology from literatue</a> as a baseline. For the two architectures, the next experiments are repeated 100 times per task:</p>\n<ol>\n<li>Solve the task with random weights (drawn from <code>Uniform(-2, 2)</code>) assigned per neuron.</li>\n<li>Solve the task after training (tuning) of random weights (drawn from <code>Uniform(-2, 2)</code>) assigned per neuron.</li>\n<li>Solve the task with a shared random weight (drawn from <code>Uniform(-2, 2)</code>).</li>\n<li>Solve the task after training (tuning) of a shared random weight (drawn from <code>Uniform(-2, 2)</code>).</li>\n</ol>\n<p>This setting makes a lot of sense to me. By comparing the performance using non-tuned random weights, we can see how dependent an architecture is on the weights. If it performs poorly, then the architecture might not be adequate for the task and relies too much on the weights. On the other hand, when tuning the weights, important things can be revealed: 1) if the architecture performed poorly with random weights but improved with tuning, then it could suggest that the tuned weights <em>hide</em> the fact that the architecture is not adequate by design; 2) if a WANN does not improve much when tuned, the topology itself might be overfitting (which is not necessarily a bad thing because then we could forget about <em>general-purpose</em> architectures and start designing <em>ad hoc</em> topologies for our tasks).</p>\n<h4 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h4><p>The reported rewards are shown in Figure 4. In short, WANNs outperform the baseline in 11/12 experiments. In general, the baseline appears very dependent on the tuned weights (very low or even negative rewards when randomly assigned). A WANN has the same drawback for random weights but it is way more robust to a random shared weight. </p>\n<p>For the rest of the analysis lets forget about the baseline. A WANN achieves its maximum performance after tuning of non-shared weights, which also improves its variance. The reward obtained with random shared weights is still low compared to the maximum obtained and shows important variance. When the shared weight is tuned, the performance becomes more certain and importantly higher but not the highest.</p>\n<img src=\"/2020/03/09/wanns/wanns-results-rl.png\" class=\"\" title=\"Figure 4: Results for the RL experiments, as reported in the original paper.\">\n\n<br/>\nWith these numbers, it is not clear yet what the essential advantage of a WANN is. To clear this out, Gaier and Ha help us with an analysis of the topologies discovered, from which I will simply rephrase the main claims. WANNs are simple and modular. They seem to ignore non-important inputs and use fewer connections than state-of-the-art approaches. An important note made in the paper is that the complexity of the topologies increases at each generation, capturing one by one the principal elements of the task. For example, in the `CartPoleSwingUp` task, a topology from an early generation captures the position-velocity relation, and in a later generation, the same topology adds nodes on top that refine the balancing (check the <a href=\"https://weightagnostic.github.io\" target=\"_blank\">interactive paper</a> for an animation of this behavior).\n\n<h3 id=\"Image-Classification\"><a href=\"#Image-Classification\" class=\"headerlink\" title=\"Image Classification\"></a>Image Classification</h3><h4 id=\"Experiment-setup-1\"><a href=\"#Experiment-setup-1\" class=\"headerlink\" title=\"Experiment setup\"></a>Experiment setup</h4><p>For Image Classification the setup changes. The task is now <a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">MNIST</a>. This time the baselines are a simple Linear Regressor and a Two-Layer CNN. Presumably, the next steps are followed 100 times too: </p>\n<ol>\n<li>Solve the task with a shared random weight (drawn from <code>Uniform(-2, 2)</code>).</li>\n<li>Solve the task after training (tuning) of a shared random weight (drawn from <code>Uniform(-2, 2)</code>).</li>\n<li>Instantiate the WNN with <code>m</code> different weights (without training) so that we end up with <code>m</code> WNNs with the same topology and just the shared weight differing. Build an ensemble where each of these networks is given a single vote so that the predicted value for an image is the digit that received the most votes.</li>\n<li>Repeat 3, but training each member of the ensemble (I assume) individually.</li>\n</ol>\n<p>A strong justification for trying with an ensemble in this task is not clear to me. My impression is that since the results did not show as strong as in RL, they experimented with an alternative that could strengthen WANNs. The task itself is simple to me, considering that a simple Two-Layer CNN is already a strong baseline for MNIST.</p>\n<h4 id=\"Results-1\"><a href=\"#Results-1\" class=\"headerlink\" title=\"Results\"></a>Results</h4><p>The results are in Figure 5. This time WANNs do not seem as well-performing as in the RL experiment. As I said, maybe thats the reason behind trying with an ensemble. I do not have much to say about this experiment, just that it seems to me that there is much room for improvement in the experiment setup. </p>\n<img src=\"/2020/03/09/wanns/wanns-results-ic.png\" class=\"\" title=\"Figure 5: Results for the Image Classification experiments, as reported in the original paper.\">\n\n<h2 id=\"My-feelings-about-the-paper\"><a href=\"#My-feelings-about-the-paper\" class=\"headerlink\" title=\"My feelings about the paper\"></a>My feelings about the paper</h2><p>I really like this paper. From the moment I knew about it, I was impressed for its simplicity and power. We are always worried about the best weights of a network and put too much care in their training to compare the architectures, but this work shows that a single shared weight can help us to compare and build architectures efficiently.</p>\n<p>My main takeover is that a WANN has a more adequate topology because forgetting about the weights and training makes the algorithm to focus on the arrangement of layers. However, I am a bit worried about the variance of the random shared weight (Figure 4). Also, I am not clear on the selection of the baseline for the RL tasks. Are they convenient? Are there better baselines? I must admit I did not read the baselines paper. If I have the time to do it, I will update this part of the summary with more insights.</p>\n<p>I like the fitness function, wich uses dominance relations. By considering different objectives, the estimation of the performance looks more solid to me. </p>\n<p>Concerning the image classification experiment, I have to say that I was expecting a bit more of discussion but it is great that Gaier and Ha spent time to show us WANNs in that domain too. To me, WANNs are not looking so powerful there, but it is also true that researchers have made much progress in image classification and sometimes is unfair for the novel approaches to compete with state-of-the-art architectures in the field, even though they have excellent ideas as it is the case of Gaier and Ha.</p>\n<p>In general, I think it would be good to see how other search methods (e.g. RL) behave with WANNs, specially because of the recent success of NAS with RL. I would also like to see the running times, because although the search does not look expensive, solving the RL environments could be. As it is the case with some papers from the Big Guys, the results look amazing but the computational resources could be out of our reach.</p>\n<p>This is all for my summary. I hope this helps you to understand and possibly implement the paper. Do not forget to share :)</p>\n"},{"title":"A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning (Garcia and Thomas)","date":"2020-06-06T08:04:11.000Z","_content":"\n# Introduction\n\nI got aware of this paper at the NeurIPS 2019 conference in Vancouver. You can find the official paper <a href=\"https://arxiv.org/abs/1902.00843\" target=\"_blank\">here</a>. The source code is hosted in <a href=\"https://github.com/fmaxgarcia/Meta-MDP\" target=\"_blank\">this GitHub repo</a>.\n\nIn this paper, Garcia and Thomas consider two aspects of reinforcement learning: exploration and exploitation. Whereas traditional RL explores randomly and learns the exploitation policy, here they learn one extra policy for exploration. That extra policy is meta-learned from a distribution of tasks (simple variations of an MDP), so that it is possible to transfer it to new tasks (from the same distribution) to guide the agent more efficiently than random exploration. Why is this interesting? Because if we learn how to explore, then exploiting on other problems should be much faster. \n\n<!-- You could think about this as learning how to perform key actions that will reveal so that later the RL agent can focus on learning the particularities of the new environment (explotation policy). -->\n\nBefore going through the paper, I recommend checking reinforcement learning concepts and approaches. In particular, you might want to check how the Q-Learning, PPO, and REINFORCE algorithhms work. An additional aspect (not strictly necessary) is meta-learning for RL, specifically Model-Agnostic Meta-Learning (MAML).\n\n# Background\n\n## Reinforcement learning: exploration and exploitation\n\nReinforcement learning (RL) is an approach to automate goal-directed learning. It relies on two entities that interact with each other: an *environment* that delivers information of its *state*, and an *agent* that using such information learns how to achieve a *goal* in the environment. The interaction is a bilateral communication where the agent performs *actions* to modify the state of the environment, which responds with a numeric *reward* measuring how good the action was to achieve the goal. Typically, the sole interest of the agent is to improve its decision-making strategy, known as the *policy*, to maximize the total reward received over the whole interaction trial. The interaction runs for a number _T_ of time-steps, in which the agent improves its policy (a mathematical model) via a _learning rule_. The longer the interaction, the better the result (or at least that is the assumption). Figure 1 illustrates this process with the Atari's Breakout.\n\n{% asset_img RL.png \"Figure 1. An example of the reinforcement learning interaction between an agent and the environment.\" %}\n\n<br/>\nWhat distinguishes a specific RL algorithm from the others is (mainly) the model for the policy (e.g., a fully-connected neural network, an RNN, a lookup table, etc.) and the _learning rule_ used to improve it. On the contrary, a characterisct they all usually share is the notion of _exploration_ and _exploitation_, which are the focus of the paper we discuss here.\n\nTo understand _exploration_ and _exploitation_ intuitively, let's think of the Atari's Breakout. Imagine that you have never played a video game in your entire life and Breakout is your first one ever. In the beginning you will have to play several times to _explore_ the game: test the possible actions you can exectue with your remote controller, see the different ways of dying, finding rewarding sequences of actions (starting point of a strategy), etc. Once you learned that, you can focus on _exploiting_ that knowledge (improving the strategy) to get a better score (reward). Each RL algorithm defines its rules for this process. I would identify two main groups: the algorithms that explicitely set an schedule for exploration/exploitation (e.g. DQN) and the ones that do not. For the latter, I would not say that exploration is inexistent, but rather it is intrinsically associated to their learning rule (e.g., gradient descent _explores_ a \"hill\" until it settles in a local optimum). Garcia and Thomas focus on explicitely scheduled exploration.\n\nThat being said, exploration can have different interests. As Garcia and Thomas mention in their paper's introduction, exploration could focus on _when_ to explore, _how much_ exploration is neded, or (the question they try to answer) _what action to take at any exploration step_. The way I understand the latter is that an agent could learn what the most insightful actions are during exploration, which is different from the most rewarding ones (happening at exploitation time).\n\n## Lifelong learning\n\nLifelong learning (for artificial intelligence) is a paradigm that aims to replicate in AI a specific behavior of human learning: we become more knowledgeable about a domain `T` with every learning experience related to it, and concurrently we improve our learning  strategy `L`. To understand this with an example, consider a neural network `nn` that should solve a set of tasks from domain `T`. From a programmatic point of view, lifelong learning could be seen as iterating over the tasks with a `nn.L(task)` function. Such a function should `return` some value `lifelong_info` that will be embedded into the network (e.g., calling `nn.improve(lifelong_info)`). Next time `nn` deals with a task of `T`, it will have a better strategy to learn and solve more efficiently and accurately.\n\nIf you want to learn more about Lifelong learning, surely you can find something useful <a href=\"https://www.cs.uic.edu/~liub/lifelong-learning.html\" target=\"_blank\">here</a>.\n\n## Meta-Learning\nWikipedia's definition for meta-data is \"data that provides information about other data\". For those familiar with web development, the _headers_ in an HTTP request might help to understand. A header is indeed meta-data; i.e., additional information that is not essential to the core of HTTP communication but it could help a website to accomplish specific functionality. It could be anything and that is the main insight I want you to take.\n\n<!-- Meta-learning (in machine learning) relies on meta-data, wh -->\n\n<!-- For the purposes of this post it is convinient to introduce _meta-learning_ as a tool that can be used to achieve lifelong learning. I like to explain meta-learning by considering first what meta-data is.  -->\n\nMeta-learning (in machine learning) relies on meta-data, which could be anything. It could be a plain timestamp of the OS, or the output of a very complex mathematical function. If you consider a task and a neural network `nn`, meta-data can be statistics from the dataset associated to the task, the value of a network's parameter at an specific iteration, the number of the iteration, an embedding at a given iteration, an aggregation of embeddings, and so on. Again, from a programmatic perspective, consider a database `DB` storing meta-data that `nn` can use and alter at any moment during the learning process `L`. Meta-learning can be seen as a function `nn.L(task, DB)`. If we are in a setting where multiple tasks are used, then `DB` is shared accross all tasks. Of course, the `DB` is just an abstraction, it could range from a simple variable changing its value within the algorithm to an RDBMS.\n\nMeta-learning is a very interesting field with some very complex approaches. I encourage you to read <a href=\"https://arxiv.org/abs/1810.03548\" target=\"_blank\">this survey by J. Vanschoren</a> to take a closer look at this amazing field.\n\n# The approach in a nutshell\n\n\n# Understanding the key elements\n\n# Experiments and results\n\n# My feelings about the paper","source":"_drafts/lifelongrl.md","raw":"---\ntitle:  A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning (Garcia and Thomas)\ndate:   2020/06/06 15:04:11 +0700\ntags: [RL, MetaLearning, NeurIPS2019]\n---\n\n# Introduction\n\nI got aware of this paper at the NeurIPS 2019 conference in Vancouver. You can find the official paper <a href=\"https://arxiv.org/abs/1902.00843\" target=\"_blank\">here</a>. The source code is hosted in <a href=\"https://github.com/fmaxgarcia/Meta-MDP\" target=\"_blank\">this GitHub repo</a>.\n\nIn this paper, Garcia and Thomas consider two aspects of reinforcement learning: exploration and exploitation. Whereas traditional RL explores randomly and learns the exploitation policy, here they learn one extra policy for exploration. That extra policy is meta-learned from a distribution of tasks (simple variations of an MDP), so that it is possible to transfer it to new tasks (from the same distribution) to guide the agent more efficiently than random exploration. Why is this interesting? Because if we learn how to explore, then exploiting on other problems should be much faster. \n\n<!-- You could think about this as learning how to perform key actions that will reveal so that later the RL agent can focus on learning the particularities of the new environment (explotation policy). -->\n\nBefore going through the paper, I recommend checking reinforcement learning concepts and approaches. In particular, you might want to check how the Q-Learning, PPO, and REINFORCE algorithhms work. An additional aspect (not strictly necessary) is meta-learning for RL, specifically Model-Agnostic Meta-Learning (MAML).\n\n# Background\n\n## Reinforcement learning: exploration and exploitation\n\nReinforcement learning (RL) is an approach to automate goal-directed learning. It relies on two entities that interact with each other: an *environment* that delivers information of its *state*, and an *agent* that using such information learns how to achieve a *goal* in the environment. The interaction is a bilateral communication where the agent performs *actions* to modify the state of the environment, which responds with a numeric *reward* measuring how good the action was to achieve the goal. Typically, the sole interest of the agent is to improve its decision-making strategy, known as the *policy*, to maximize the total reward received over the whole interaction trial. The interaction runs for a number _T_ of time-steps, in which the agent improves its policy (a mathematical model) via a _learning rule_. The longer the interaction, the better the result (or at least that is the assumption). Figure 1 illustrates this process with the Atari's Breakout.\n\n{% asset_img RL.png \"Figure 1. An example of the reinforcement learning interaction between an agent and the environment.\" %}\n\n<br/>\nWhat distinguishes a specific RL algorithm from the others is (mainly) the model for the policy (e.g., a fully-connected neural network, an RNN, a lookup table, etc.) and the _learning rule_ used to improve it. On the contrary, a characterisct they all usually share is the notion of _exploration_ and _exploitation_, which are the focus of the paper we discuss here.\n\nTo understand _exploration_ and _exploitation_ intuitively, let's think of the Atari's Breakout. Imagine that you have never played a video game in your entire life and Breakout is your first one ever. In the beginning you will have to play several times to _explore_ the game: test the possible actions you can exectue with your remote controller, see the different ways of dying, finding rewarding sequences of actions (starting point of a strategy), etc. Once you learned that, you can focus on _exploiting_ that knowledge (improving the strategy) to get a better score (reward). Each RL algorithm defines its rules for this process. I would identify two main groups: the algorithms that explicitely set an schedule for exploration/exploitation (e.g. DQN) and the ones that do not. For the latter, I would not say that exploration is inexistent, but rather it is intrinsically associated to their learning rule (e.g., gradient descent _explores_ a \"hill\" until it settles in a local optimum). Garcia and Thomas focus on explicitely scheduled exploration.\n\nThat being said, exploration can have different interests. As Garcia and Thomas mention in their paper's introduction, exploration could focus on _when_ to explore, _how much_ exploration is neded, or (the question they try to answer) _what action to take at any exploration step_. The way I understand the latter is that an agent could learn what the most insightful actions are during exploration, which is different from the most rewarding ones (happening at exploitation time).\n\n## Lifelong learning\n\nLifelong learning (for artificial intelligence) is a paradigm that aims to replicate in AI a specific behavior of human learning: we become more knowledgeable about a domain `T` with every learning experience related to it, and concurrently we improve our learning  strategy `L`. To understand this with an example, consider a neural network `nn` that should solve a set of tasks from domain `T`. From a programmatic point of view, lifelong learning could be seen as iterating over the tasks with a `nn.L(task)` function. Such a function should `return` some value `lifelong_info` that will be embedded into the network (e.g., calling `nn.improve(lifelong_info)`). Next time `nn` deals with a task of `T`, it will have a better strategy to learn and solve more efficiently and accurately.\n\nIf you want to learn more about Lifelong learning, surely you can find something useful <a href=\"https://www.cs.uic.edu/~liub/lifelong-learning.html\" target=\"_blank\">here</a>.\n\n## Meta-Learning\nWikipedia's definition for meta-data is \"data that provides information about other data\". For those familiar with web development, the _headers_ in an HTTP request might help to understand. A header is indeed meta-data; i.e., additional information that is not essential to the core of HTTP communication but it could help a website to accomplish specific functionality. It could be anything and that is the main insight I want you to take.\n\n<!-- Meta-learning (in machine learning) relies on meta-data, wh -->\n\n<!-- For the purposes of this post it is convinient to introduce _meta-learning_ as a tool that can be used to achieve lifelong learning. I like to explain meta-learning by considering first what meta-data is.  -->\n\nMeta-learning (in machine learning) relies on meta-data, which could be anything. It could be a plain timestamp of the OS, or the output of a very complex mathematical function. If you consider a task and a neural network `nn`, meta-data can be statistics from the dataset associated to the task, the value of a network's parameter at an specific iteration, the number of the iteration, an embedding at a given iteration, an aggregation of embeddings, and so on. Again, from a programmatic perspective, consider a database `DB` storing meta-data that `nn` can use and alter at any moment during the learning process `L`. Meta-learning can be seen as a function `nn.L(task, DB)`. If we are in a setting where multiple tasks are used, then `DB` is shared accross all tasks. Of course, the `DB` is just an abstraction, it could range from a simple variable changing its value within the algorithm to an RDBMS.\n\nMeta-learning is a very interesting field with some very complex approaches. I encourage you to read <a href=\"https://arxiv.org/abs/1810.03548\" target=\"_blank\">this survey by J. Vanschoren</a> to take a closer look at this amazing field.\n\n# The approach in a nutshell\n\n\n# Understanding the key elements\n\n# Experiments and results\n\n# My feelings about the paper","slug":"lifelongrl","published":0,"updated":"2020-06-06T20:08:08.821Z","_id":"ckb42jacj0000ib59fh9c0hb3","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>I got aware of this paper at the NeurIPS 2019 conference in Vancouver. You can find the official paper <a href=\"https://arxiv.org/abs/1902.00843\" target=\"_blank\">here</a>. The source code is hosted in <a href=\"https://github.com/fmaxgarcia/Meta-MDP\" target=\"_blank\">this GitHub repo</a>.</p>\n<p>In this paper, Garcia and Thomas consider two aspects of reinforcement learning: exploration and exploitation. Whereas traditional RL explores randomly and learns the exploitation policy, here they learn one extra policy for exploration. That extra policy is meta-learned from a distribution of tasks (simple variations of an MDP), so that it is possible to transfer it to new tasks (from the same distribution) to guide the agent more efficiently than random exploration. Why is this interesting? Because if we learn how to explore, then exploiting on other problems should be much faster. </p>\n<!-- You could think about this as learning how to perform key actions that will reveal so that later the RL agent can focus on learning the particularities of the new environment (explotation policy). -->\n\n<p>Before going through the paper, I recommend checking reinforcement learning concepts and approaches. In particular, you might want to check how the Q-Learning, PPO, and REINFORCE algorithhms work. An additional aspect (not strictly necessary) is meta-learning for RL, specifically Model-Agnostic Meta-Learning (MAML).</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><h2 id=\"Reinforcement-learning-exploration-and-exploitation\"><a href=\"#Reinforcement-learning-exploration-and-exploitation\" class=\"headerlink\" title=\"Reinforcement learning: exploration and exploitation\"></a>Reinforcement learning: exploration and exploitation</h2><p>Reinforcement learning (RL) is an approach to automate goal-directed learning. It relies on two entities that interact with each other: an <em>environment</em> that delivers information of its <em>state</em>, and an <em>agent</em> that using such information learns how to achieve a <em>goal</em> in the environment. The interaction is a bilateral communication where the agent performs <em>actions</em> to modify the state of the environment, which responds with a numeric <em>reward</em> measuring how good the action was to achieve the goal. Typically, the sole interest of the agent is to improve its decision-making strategy, known as the <em>policy</em>, to maximize the total reward received over the whole interaction trial. The interaction runs for a number <em>T</em> of time-steps, in which the agent improves its policy (a mathematical model) via a <em>learning rule</em>. The longer the interaction, the better the result (or at least that is the assumption). Figure 1 illustrates this process with the Ataris Breakout.</p>\n<img src=\"/2020/06/06/lifelongrl/RL.png\" class=\"\" title=\"Figure 1. An example of the reinforcement learning interaction between an agent and the environment.\">\n\n<br/>\nWhat distinguishes a specific RL algorithm from the others is (mainly) the model for the policy (e.g., a fully-connected neural network, an RNN, a lookup table, etc.) and the _learning rule_ used to improve it. On the contrary, a characterisct they all usually share is the notion of _exploration_ and _exploitation_, which are the focus of the paper we discuss here.\n\n<p>To understand <em>exploration</em> and <em>exploitation</em> intuitively, lets think of the Ataris Breakout. Imagine that you have never played a video game in your entire life and Breakout is your first one ever. In the beginning you will have to play several times to <em>explore</em> the game: test the possible actions you can exectue with your remote controller, see the different ways of dying, finding rewarding sequences of actions (starting point of a strategy), etc. Once you learned that, you can focus on <em>exploiting</em> that knowledge (improving the strategy) to get a better score (reward). Each RL algorithm defines its rules for this process. I would identify two main groups: the algorithms that explicitely set an schedule for exploration/exploitation (e.g. DQN) and the ones that do not. For the latter, I would not say that exploration is inexistent, but rather it is intrinsically associated to their learning rule (e.g., gradient descent <em>explores</em> a hill until it settles in a local optimum). Garcia and Thomas focus on explicitely scheduled exploration.</p>\n<p>That being said, exploration can have different interests. As Garcia and Thomas mention in their papers introduction, exploration could focus on <em>when</em> to explore, <em>how much</em> exploration is neded, or (the question they try to answer) <em>what action to take at any exploration step</em>. The way I understand the latter is that an agent could learn what the most insightful actions are during exploration, which is different from the most rewarding ones (happening at exploitation time).</p>\n<h2 id=\"Lifelong-learning\"><a href=\"#Lifelong-learning\" class=\"headerlink\" title=\"Lifelong learning\"></a>Lifelong learning</h2><p>Lifelong learning (for artificial intelligence) is a paradigm that aims to replicate in AI a specific behavior of human learning: we become more knowledgeable about a domain <code>T</code> with every learning experience related to it, and concurrently we improve our learning  strategy <code>L</code>. To understand this with an example, consider a neural network <code>nn</code> that should solve a set of tasks from domain <code>T</code>. From a programmatic point of view, lifelong learning could be seen as iterating over the tasks with a <code>nn.L(task)</code> function. Such a function should <code>return</code> some value <code>lifelong_info</code> that will be embedded into the network (e.g., calling <code>nn.improve(lifelong_info)</code>). Next time <code>nn</code> deals with a task of <code>T</code>, it will have a better strategy to learn and solve more efficiently and accurately.</p>\n<p>If you want to learn more about Lifelong learning, surely you can find something useful <a href=\"https://www.cs.uic.edu/~liub/lifelong-learning.html\" target=\"_blank\">here</a>.</p>\n<h2 id=\"Meta-Learning\"><a href=\"#Meta-Learning\" class=\"headerlink\" title=\"Meta-Learning\"></a>Meta-Learning</h2><p>Wikipedias definition for meta-data is data that provides information about other data. For those familiar with web development, the <em>headers</em> in an HTTP request might help to understand. A header is indeed meta-data; i.e., additional information that is not essential to the core of HTTP communication but it could help a website to accomplish specific functionality. It could be anything and that is the main insight I want you to take.</p>\n<!-- Meta-learning (in machine learning) relies on meta-data, wh -->\n\n<!-- For the purposes of this post it is convinient to introduce _meta-learning_ as a tool that can be used to achieve lifelong learning. I like to explain meta-learning by considering first what meta-data is.  -->\n\n<p>Meta-learning (in machine learning) relies on meta-data, which could be anything. It could be a plain timestamp of the OS, or the output of a very complex mathematical function. If you consider a task and a neural network <code>nn</code>, meta-data can be statistics from the dataset associated to the task, the value of a networks parameter at an specific iteration, the number of the iteration, an embedding at a given iteration, an aggregation of embeddings, and so on. Again, from a programmatic perspective, consider a database <code>DB</code> storing meta-data that <code>nn</code> can use and alter at any moment during the learning process <code>L</code>. Meta-learning can be seen as a function <code>nn.L(task, DB)</code>. If we are in a setting where multiple tasks are used, then <code>DB</code> is shared accross all tasks. Of course, the <code>DB</code> is just an abstraction, it could range from a simple variable changing its value within the algorithm to an RDBMS.</p>\n<p>Meta-learning is a very interesting field with some very complex approaches. I encourage you to read <a href=\"https://arxiv.org/abs/1810.03548\" target=\"_blank\">this survey by J. Vanschoren</a> to take a closer look at this amazing field.</p>\n<h1 id=\"The-approach-in-a-nutshell\"><a href=\"#The-approach-in-a-nutshell\" class=\"headerlink\" title=\"The approach in a nutshell\"></a>The approach in a nutshell</h1><h1 id=\"Understanding-the-key-elements\"><a href=\"#Understanding-the-key-elements\" class=\"headerlink\" title=\"Understanding the key elements\"></a>Understanding the key elements</h1><h1 id=\"Experiments-and-results\"><a href=\"#Experiments-and-results\" class=\"headerlink\" title=\"Experiments and results\"></a>Experiments and results</h1><h1 id=\"My-feelings-about-the-paper\"><a href=\"#My-feelings-about-the-paper\" class=\"headerlink\" title=\"My feelings about the paper\"></a>My feelings about the paper</h1>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>I got aware of this paper at the NeurIPS 2019 conference in Vancouver. You can find the official paper <a href=\"https://arxiv.org/abs/1902.00843\" target=\"_blank\">here</a>. The source code is hosted in <a href=\"https://github.com/fmaxgarcia/Meta-MDP\" target=\"_blank\">this GitHub repo</a>.</p>\n<p>In this paper, Garcia and Thomas consider two aspects of reinforcement learning: exploration and exploitation. Whereas traditional RL explores randomly and learns the exploitation policy, here they learn one extra policy for exploration. That extra policy is meta-learned from a distribution of tasks (simple variations of an MDP), so that it is possible to transfer it to new tasks (from the same distribution) to guide the agent more efficiently than random exploration. Why is this interesting? Because if we learn how to explore, then exploiting on other problems should be much faster. </p>\n<!-- You could think about this as learning how to perform key actions that will reveal so that later the RL agent can focus on learning the particularities of the new environment (explotation policy). -->\n\n<p>Before going through the paper, I recommend checking reinforcement learning concepts and approaches. In particular, you might want to check how the Q-Learning, PPO, and REINFORCE algorithhms work. An additional aspect (not strictly necessary) is meta-learning for RL, specifically Model-Agnostic Meta-Learning (MAML).</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><h2 id=\"Reinforcement-learning-exploration-and-exploitation\"><a href=\"#Reinforcement-learning-exploration-and-exploitation\" class=\"headerlink\" title=\"Reinforcement learning: exploration and exploitation\"></a>Reinforcement learning: exploration and exploitation</h2><p>Reinforcement learning (RL) is an approach to automate goal-directed learning. It relies on two entities that interact with each other: an <em>environment</em> that delivers information of its <em>state</em>, and an <em>agent</em> that using such information learns how to achieve a <em>goal</em> in the environment. The interaction is a bilateral communication where the agent performs <em>actions</em> to modify the state of the environment, which responds with a numeric <em>reward</em> measuring how good the action was to achieve the goal. Typically, the sole interest of the agent is to improve its decision-making strategy, known as the <em>policy</em>, to maximize the total reward received over the whole interaction trial. The interaction runs for a number <em>T</em> of time-steps, in which the agent improves its policy (a mathematical model) via a <em>learning rule</em>. The longer the interaction, the better the result (or at least that is the assumption). Figure 1 illustrates this process with the Ataris Breakout.</p>\n<img src=\"/2020/06/06/lifelongrl/RL.png\" class=\"\" title=\"Figure 1. An example of the reinforcement learning interaction between an agent and the environment.\">\n\n<br/>\nWhat distinguishes a specific RL algorithm from the others is (mainly) the model for the policy (e.g., a fully-connected neural network, an RNN, a lookup table, etc.) and the _learning rule_ used to improve it. On the contrary, a characterisct they all usually share is the notion of _exploration_ and _exploitation_, which are the focus of the paper we discuss here.\n\n<p>To understand <em>exploration</em> and <em>exploitation</em> intuitively, lets think of the Ataris Breakout. Imagine that you have never played a video game in your entire life and Breakout is your first one ever. In the beginning you will have to play several times to <em>explore</em> the game: test the possible actions you can exectue with your remote controller, see the different ways of dying, finding rewarding sequences of actions (starting point of a strategy), etc. Once you learned that, you can focus on <em>exploiting</em> that knowledge (improving the strategy) to get a better score (reward). Each RL algorithm defines its rules for this process. I would identify two main groups: the algorithms that explicitely set an schedule for exploration/exploitation (e.g. DQN) and the ones that do not. For the latter, I would not say that exploration is inexistent, but rather it is intrinsically associated to their learning rule (e.g., gradient descent <em>explores</em> a hill until it settles in a local optimum). Garcia and Thomas focus on explicitely scheduled exploration.</p>\n<p>That being said, exploration can have different interests. As Garcia and Thomas mention in their papers introduction, exploration could focus on <em>when</em> to explore, <em>how much</em> exploration is neded, or (the question they try to answer) <em>what action to take at any exploration step</em>. The way I understand the latter is that an agent could learn what the most insightful actions are during exploration, which is different from the most rewarding ones (happening at exploitation time).</p>\n<h2 id=\"Lifelong-learning\"><a href=\"#Lifelong-learning\" class=\"headerlink\" title=\"Lifelong learning\"></a>Lifelong learning</h2><p>Lifelong learning (for artificial intelligence) is a paradigm that aims to replicate in AI a specific behavior of human learning: we become more knowledgeable about a domain <code>T</code> with every learning experience related to it, and concurrently we improve our learning  strategy <code>L</code>. To understand this with an example, consider a neural network <code>nn</code> that should solve a set of tasks from domain <code>T</code>. From a programmatic point of view, lifelong learning could be seen as iterating over the tasks with a <code>nn.L(task)</code> function. Such a function should <code>return</code> some value <code>lifelong_info</code> that will be embedded into the network (e.g., calling <code>nn.improve(lifelong_info)</code>). Next time <code>nn</code> deals with a task of <code>T</code>, it will have a better strategy to learn and solve more efficiently and accurately.</p>\n<p>If you want to learn more about Lifelong learning, surely you can find something useful <a href=\"https://www.cs.uic.edu/~liub/lifelong-learning.html\" target=\"_blank\">here</a>.</p>\n<h2 id=\"Meta-Learning\"><a href=\"#Meta-Learning\" class=\"headerlink\" title=\"Meta-Learning\"></a>Meta-Learning</h2><p>Wikipedias definition for meta-data is data that provides information about other data. For those familiar with web development, the <em>headers</em> in an HTTP request might help to understand. A header is indeed meta-data; i.e., additional information that is not essential to the core of HTTP communication but it could help a website to accomplish specific functionality. It could be anything and that is the main insight I want you to take.</p>\n<!-- Meta-learning (in machine learning) relies on meta-data, wh -->\n\n<!-- For the purposes of this post it is convinient to introduce _meta-learning_ as a tool that can be used to achieve lifelong learning. I like to explain meta-learning by considering first what meta-data is.  -->\n\n<p>Meta-learning (in machine learning) relies on meta-data, which could be anything. It could be a plain timestamp of the OS, or the output of a very complex mathematical function. If you consider a task and a neural network <code>nn</code>, meta-data can be statistics from the dataset associated to the task, the value of a networks parameter at an specific iteration, the number of the iteration, an embedding at a given iteration, an aggregation of embeddings, and so on. Again, from a programmatic perspective, consider a database <code>DB</code> storing meta-data that <code>nn</code> can use and alter at any moment during the learning process <code>L</code>. Meta-learning can be seen as a function <code>nn.L(task, DB)</code>. If we are in a setting where multiple tasks are used, then <code>DB</code> is shared accross all tasks. Of course, the <code>DB</code> is just an abstraction, it could range from a simple variable changing its value within the algorithm to an RDBMS.</p>\n<p>Meta-learning is a very interesting field with some very complex approaches. I encourage you to read <a href=\"https://arxiv.org/abs/1810.03548\" target=\"_blank\">this survey by J. Vanschoren</a> to take a closer look at this amazing field.</p>\n<h1 id=\"The-approach-in-a-nutshell\"><a href=\"#The-approach-in-a-nutshell\" class=\"headerlink\" title=\"The approach in a nutshell\"></a>The approach in a nutshell</h1><h1 id=\"Understanding-the-key-elements\"><a href=\"#Understanding-the-key-elements\" class=\"headerlink\" title=\"Understanding the key elements\"></a>Understanding the key elements</h1><h1 id=\"Experiments-and-results\"><a href=\"#Experiments-and-results\" class=\"headerlink\" title=\"Experiments and results\"></a>Experiments and results</h1><h1 id=\"My-feelings-about-the-paper\"><a href=\"#My-feelings-about-the-paper\" class=\"headerlink\" title=\"My feelings about the paper\"></a>My feelings about the paper</h1>"}],"PostAsset":[{"_id":"source/_posts/2020-06-06-wanns/wanns-results-rl.png","slug":"wanns-results-rl.png","post":"ckb41ifb9000080592xcg1xl8","modified":0,"renderable":0},{"_id":"source/_posts/2020-06-06-wanns/wanns-minimal-net.png","slug":"wanns-minimal-net.png","post":"ckb41ifb9000080592xcg1xl8","modified":0,"renderable":0},{"_id":"source/_posts/2020-06-06-wanns/wanns-neat-crossover.png","slug":"wanns-neat-crossover.png","post":"ckb41ifb9000080592xcg1xl8","modified":0,"renderable":0},{"_id":"source/_posts/2020-06-06-wanns/wanns-neat-genome.png","slug":"wanns-neat-genome.png","post":"ckb41ifb9000080592xcg1xl8","modified":0,"renderable":0},{"_id":"source/_posts/2020-06-06-wanns/wanns-results-ic.png","slug":"wanns-results-ic.png","post":"ckb41ifb9000080592xcg1xl8","modified":0,"renderable":0},{"_id":"source/_drafts/lifelongrl/RL.png","slug":"RL.png","post":"ckb42jacj0000ib59fh9c0hb3","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"ckb41ifb9000080592xcg1xl8","tag_id":"ckb40s3gi000dn159h4hu93os","_id":"ckb41ifbj000180596osp5xrb"},{"post_id":"ckb41ifb9000080592xcg1xl8","tag_id":"ckb40s3gi000en159ffs2bozy","_id":"ckb41ifbj00028059dfcoh8cy"},{"post_id":"ckb41ifb9000080592xcg1xl8","tag_id":"ckb40s3gj000fn1594y5i7l19","_id":"ckb41ifbk000380590aox8xej"},{"post_id":"ckb41ifb9000080592xcg1xl8","tag_id":"ckb40s3gj000gn159gg7t9vle","_id":"ckb41ifbk00048059dx099vi5"},{"post_id":"ckb41ifb9000080592xcg1xl8","tag_id":"ckb40s3gk000hn1596y8a7fm9","_id":"ckb41ifbl000580591oezacf7"},{"post_id":"ckb42jacj0000ib59fh9c0hb3","tag_id":"ckb40s3gj000fn1594y5i7l19","_id":"ckb42jacr0002ib59homb0os3"},{"post_id":"ckb42jacj0000ib59fh9c0hb3","tag_id":"ckb42jacp0001ib59bqz579dx","_id":"ckb42jacr0003ib5983kbfbls"},{"post_id":"ckb42jacj0000ib59fh9c0hb3","tag_id":"ckb40s3gk000hn1596y8a7fm9","_id":"ckb42jacr0004ib591kvp4xpq"}],"Tag":[{"name":"NAS, EvolutionaryAlgorithms, RL, ImageClassification, NeurIPS2019]","_id":"ckb40rswj000bn159hicdfh96"},{"name":"NAS","_id":"ckb40s3gi000dn159h4hu93os"},{"name":"EvolutionaryAlgorithms","_id":"ckb40s3gi000en159ffs2bozy"},{"name":"RL","_id":"ckb40s3gj000fn1594y5i7l19"},{"name":"ImageClassification","_id":"ckb40s3gj000gn159gg7t9vle"},{"name":"NeurIPS2019","_id":"ckb40s3gk000hn1596y8a7fm9"},{"name":"MetaLearning","_id":"ckb42jacp0001ib59bqz579dx"}]}}